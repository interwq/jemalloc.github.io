From yasaswinig7 at gmail.com  Tue Mar  8 11:33:40 2016
From: yasaswinig7 at gmail.com (Yasaswini G)
Date: Tue, 8 Mar 2016 11:33:40 -0800
Subject: Interpretation of bit patters in arena_chunk_map_bits_s
Message-ID: <CAL8RdmTwU2tF59fpmzwtjNHBeaBW47LCmYB+i8x+pfdcnBCNDw@mail.gmail.com>

Hi,

What does the three patterns signify under each one of the allocation type
(unallocated(dirty/clean), small, large)  in "arena_chunk_map_bits_s"

I understand the allocation types (unallocated, small, large),  but I am
not sure why there are three different patters under each allocation type.

For example under Unallocated (clean), we have following bit patterns:
     *Unallocated (clean):*
     *     ssssssss ssssssss sss+++++ +++dum-a         - pattern1
     *     xxxxxxxx xxxxxxxx xxxxxxxx xxx-Uxxx             - pattern2
     *     ssssssss ssssssss sss+++++ +++dUm-a         - pattern3

According to pattern2, if bit3 is set (U) then remaining bits are don't
care, but according to pattern3, when bit3 is set (U), bit 13 - bit 31
(s....s) is still valid as run_size.

The comments inside struct are elaborate, but I am still confused. May be I
am missing something. I am trying to understand the logical interpretation
of each one of those patterns!  Appreciate, if someone could explain with
an example/direct me to a reference point.

Thanks
Yasaswini
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20160308/65b2ec2b/attachment.html>

From jasone at canonware.com  Tue Mar  8 13:09:46 2016
From: jasone at canonware.com (Jason Evans)
Date: Tue, 8 Mar 2016 13:09:46 -0800
Subject: Interpretation of bit patters in arena_chunk_map_bits_s
In-Reply-To: <CAL8RdmTwU2tF59fpmzwtjNHBeaBW47LCmYB+i8x+pfdcnBCNDw@mail.gmail.com>
References: <CAL8RdmTwU2tF59fpmzwtjNHBeaBW47LCmYB+i8x+pfdcnBCNDw@mail.gmail.com>
Message-ID: <BC422F63-147F-403C-A5B0-DD35B608439D@canonware.com>

On Mar 8, 2016, at 11:33 AM, Yasaswini G <yasaswinig7 at gmail.com> wrote:
> Hi,
> 
> What does the three patterns signify under each one of the allocation type (unallocated(dirty/clean), small, large)  in "arena_chunk_map_bits_s"
> 
> I understand the allocation types (unallocated, small, large),  but I am not sure why there are three different patters under each allocation type.
> 
> For example under Unallocated (clean), we have following bit patterns: 
>      Unallocated (clean):
>      *     ssssssss ssssssss sss+++++ +++dum-a         - pattern1

The above corresponds to the first page within a run.

>      *     xxxxxxxx xxxxxxxx xxxxxxxx xxx-Uxxx             - pattern2

The above corresponds to an interior page within a run.

>      *     ssssssss ssssssss sss+++++ +++dUm-a         - pattern3

The above corresponds to the last page within a run.

Jason
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20160308/36057860/attachment.html>

From Michael.Schenk at albistechnologies.com  Thu Mar 10 02:49:33 2016
From: Michael.Schenk at albistechnologies.com (Schenk, Michael)
Date: Thu, 10 Mar 2016 10:49:33 +0000
Subject: SIGBUS issue with jemalloc-4.1.0 on SH4 platform
Message-ID: <DB5PR05MB10489AF55B8C8AEC0FF9A26891B40@DB5PR05MB1048.eurprd05.prod.outlook.com>

Dear Jason, dear friends,

I'm a big fan of JEMALLOC and we use it since years in our products but running into problems from time to time when upgrade to newer
versions. Currently we run (stable) with 4.0.4 on our SH-4 platform. Yesterday I have done some tests with the 4.1.0 and hit some SIGBUS issues during updating the software on my embedded devices.
The crash looks like:

crasher:  /proc/self/maps: 298c7000-298d7000 ---p 0000b000 00:0f 2312       /tmp/root/libgcc_s-4.8.4.so.1
crasher:  /proc/self/maps: 298d7000-298d8000 rw-p 0000b000 00:0f 2312       /tmp/root/libgcc_s-4.8.4.so.1
crasher:  /proc/self/maps: 298d8000-298da000 rw-p 00000000 00:00 0
crasher:  /proc/self/maps: 29a00000-29e02000 rw-p 00000000 00:00 0          [stack:2071]
crasher:  /proc/self/maps: 2dce4000-2dce5000 rw-p 00000000 00:00 0
crasher:  /proc/self/maps: 7b84a000-7b86c000 rw-p 00000000 00:00 0
crasher:BACKTRACE:
crasher: #0 stacktrace + 0x44 [ip=0x2960e400] [sp=0x29c534f0]
crasher: #1 stacktrace + 0x256 [ip=0x2960e612] [sp=0x29c577f8]
crasher: #2 <unknown symbol> [ip=0x29c579ac] [sp=0x29c57828]
crasher: #3 calloc + 0x5ac [ip=0x29589024] [sp=0x7b86a718]
Bus error
Error updating the software

In the past we discovered exactly the same issue on 4.0.0 which then was fixed in the 4.0.1 release. It's hard to get more traces because when we compile jemalloc with debug the crash did not happen.

Config for normal build:
jemalloc version   : 4.1.0-0-gdf900dbfaf4835d3efc06d771535f3e781544913
library revision   : 2

CONFIG             : --target=sh4-linux --host=sh4-linux --prefix=/work/galileo-8xxx/libs/jemalloc/install host_alias=sh4-linux target_alias=sh4-linux
CC                 : sh4-linux-gcc
CFLAGS             : -std=gnu99 -Wall -Werror=declaration-after-statement -pipe -g3 -fvisibility=hidden -O3 -funroll-loops
CPPFLAGS           :  -D_GNU_SOURCE -D_REENTRANT
LDFLAGS            :
EXTRA_LDFLAGS      :
LIBS               :  -lpthread
TESTLIBS           : -lrt
RPATH_EXTRA        :

XSLTPROC           : /usr/bin/xsltproc
XSLROOT            : /usr/share/sgml/docbook/xsl-stylesheets

PREFIX             : /work/galileo-8xxx/libs/jemalloc/install
BINDIR             : /work/galileo-8xxx/libs/jemalloc/install/bin
DATADIR            : /work/galileo-8xxx/libs/jemalloc/install/share
INCLUDEDIR         : /work/galileo-8xxx/libs/jemalloc/install/include
LIBDIR             : /work/galileo-8xxx/libs/jemalloc/install/lib
MANDIR             : /work/galileo-8xxx/libs/jemalloc/install/share/man

srcroot            :
abs_srcroot        : /work/galileo-8xxx/libs/jemalloc/jemalloc-4.1.0/
objroot            :
abs_objroot        : /work/galileo-8xxx/libs/jemalloc/jemalloc-4.1.0/

JEMALLOC_PREFIX    :
JEMALLOC_PRIVATE_NAMESPACE
                   : je_
install_suffix     :
malloc_conf        :
autogen            : 0
cc-silence         : 1
debug              : 0
code-coverage      : 0
stats              : 1
prof               : 0
prof-libunwind     : 0
prof-libgcc        : 0
prof-gcc           : 0
tcache             : 1
fill               : 1
utrace             : 0
valgrind           : 0
xmalloc            : 0
munmap             : 0
lazy_lock          : 0
tls                : 1
cache-oblivious    : 1


Config for debug build
jemalloc version   : 4.1.0-0-gdf900dbfaf4835d3efc06d771535f3e781544913
library revision   : 2

CONFIG             : --target=sh4-linux --host=sh4-linux --enable-debug --enable-prof --enable-prof-libunwind --prefix=/work/galileo-8xxx/libs/jemalloc/install host_alias=sh4-linux target_alias=sh4-linux
CC                 : sh4-linux-gcc
CFLAGS             : -std=gnu99 -Wall -Werror=declaration-after-statement -pipe -g3 -fvisibility=hidden
CPPFLAGS           :  -D_GNU_SOURCE -D_REENTRANT
LDFLAGS            :
EXTRA_LDFLAGS      :
LIBS               :  -lunwind -lm -lpthread
TESTLIBS           : -lrt
RPATH_EXTRA        :

XSLTPROC           : /usr/bin/xsltproc
XSLROOT            : /usr/share/sgml/docbook/xsl-stylesheets

PREFIX             : /work/galileo-8xxx/libs/jemalloc/install
BINDIR             : /work/galileo-8xxx/libs/jemalloc/install/bin
DATADIR            : /work/galileo-8xxx/libs/jemalloc/install/share
INCLUDEDIR         : /work/galileo-8xxx/libs/jemalloc/install/include
LIBDIR             : /work/galileo-8xxx/libs/jemalloc/install/lib
MANDIR             : /work/galileo-8xxx/libs/jemalloc/install/share/man

srcroot            :
abs_srcroot        : /work/galileo-8xxx/libs/jemalloc/jemalloc-4.1.0/
objroot            :
abs_objroot        : /work/galileo-8xxx/libs/jemalloc/jemalloc-4.1.0/

JEMALLOC_PREFIX    :
JEMALLOC_PRIVATE_NAMESPACE
                   : je_
install_suffix     :
malloc_conf        :
autogen            : 0
cc-silence         : 1
debug              : 1
code-coverage      : 0
stats              : 1
prof               : 1
prof-libunwind     : 1
prof-libgcc        : 0
prof-gcc           : 0
tcache             : 1
fill               : 1
utrace             : 0
valgrind           : 0
xmalloc            : 0
munmap             : 0
lazy_lock          : 0
tls                : 1
cache-oblivious    : 1

Any idea how I can debug this further or any hint what't going wrong ?

Many thanks in advanced and kind regards

Michael

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20160310/2f5c5eab/attachment-0001.html>

From linuxhippy at gmail.com  Mon Mar 14 10:52:43 2016
From: linuxhippy at gmail.com (Clemens Eisserer)
Date: Mon, 14 Mar 2016 18:52:43 +0100
Subject: Tuning-advise for soft-realtime systems
Message-ID: <CAFvQSYSmuWSfPFT7wCrMS7VzjetB_DqptEMNjpN=J21X5W1B2w@mail.gmail.com>

Hello,

I am currently evaluating jemalloc for a soft-realtime system where
worst-case latency is of concern.
When allocating areas of random size honoring the size-distribution
which is to be expected, usually latency is of no concern.
However, from time to time there are latency spikes of ~300
microseconds for a single malloc especially for mid-sized allocations
(e.g. 1024byte).

My guess is those latency spikes arise when jemalloc requests
additional memory for it's arena's from the kernel.
Are there any tuning knobs to reduce the duration of those spikes and
instead increasing there frequency?

Thank you in advance, Clemens

From jasone at canonware.com  Tue Mar 15 11:27:57 2016
From: jasone at canonware.com (Jason Evans)
Date: Tue, 15 Mar 2016 11:27:57 -0700
Subject: Tuning-advise for soft-realtime systems
In-Reply-To: <CAFvQSYSmuWSfPFT7wCrMS7VzjetB_DqptEMNjpN=J21X5W1B2w@mail.gmail.com>
References: <CAFvQSYSmuWSfPFT7wCrMS7VzjetB_DqptEMNjpN=J21X5W1B2w@mail.gmail.com>
Message-ID: <2B39F4D4-0753-4B6C-9063-88A480C2A4DA@canonware.com>

On Mar 14, 2016, at 10:52 AM, Clemens Eisserer <linuxhippy at gmail.com> wrote:
> I am currently evaluating jemalloc for a soft-realtime system where
> worst-case latency is of concern.
> When allocating areas of random size honoring the size-distribution
> which is to be expected, usually latency is of no concern.
> However, from time to time there are latency spikes of ~300
> microseconds for a single malloc especially for mid-sized allocations
> (e.g. 1024byte).
> 
> My guess is those latency spikes arise when jemalloc requests
> additional memory for it's arena's from the kernel.
> Are there any tuning knobs to reduce the duration of those spikes and
> instead increasing there frequency?

The top suggestions I can think of are to disable unused dirty page purging, and to increase the chunk size to the largest size you can tolerate in terms of memory usage.

Jason

From jeffery.griffith at gmail.com  Thu Mar 17 09:17:52 2016
From: jeffery.griffith at gmail.com (Jeffery Griffith)
Date: Thu, 17 Mar 2016 12:17:52 -0400
Subject: Invalid conf pair
Message-ID: <CALodpJ1JAnoDCqKSq1M_QMBziL751H+7z5r=_jQafVDWeV-onQ@mail.gmail.com>

My first attempt at trying to integrate jemalloc and I can't get out of the
starting blocks :-)  From what I read, it seems that I should be able to
get jemalloc into the JVM with LD_PRELOAD and MALLOC_CONF as used below. I
took the examples straight from the wiki but I must be missing something
incredibly basic. I built the .so straight from the download with no
problems (version 4.0) and my JVM does seem to run with it with no problem
(radically lower virtual size and i can see the .so is loaded with pmap),
but I clearly don't know what the hell I'm doing with MALLOC_CONF.
TIA,
--jg


jgriffith at mydevbox.c1.:~/JEMALLOC$ echo $LD_PRELOAD
/usr/local/lib/libjemalloc.so
jgriffith at mydevbox.c1.:~/JEMALLOC$ echo $MALLOC_CONF
prof:true,lg_prof_interval:30,lg_prof_sample:17
jgriffith at mydevbox.c1.:~/JEMALLOC$ java -jar allocmemory.jar
<jemalloc>: Invalid conf pair: prof:true
<jemalloc>: Invalid conf pair: lg_prof_interval:30
<jemalloc>: Invalid conf pair: lg_prof_sample:17
<jemalloc>: Invalid conf pair: prof:true
<jemalloc>: Invalid conf pair: lg_prof_interval:30
<jemalloc>: Invalid conf pair: lg_prof_sample:17
<jemalloc>: Invalid conf pair: prof:true
<jemalloc>: Invalid conf pair: lg_prof_interval:30
<jemalloc>: Invalid conf pair: lg_prof_sample:17
<jemalloc>: Invalid conf pair: prof:true
<jemalloc>: Invalid conf pair: lg_prof_interval:30
<jemalloc>: Invalid conf pair: lg_prof_sample:17
^C
jgriffith at mydevbox.c1.:~/JEMALLOC$ ls
<jemalloc>: Invalid conf pair: prof:true
<jemalloc>: Invalid conf pair: lg_prof_interval:30
<jemalloc>: Invalid conf pair: lg_prof_sample:17
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20160317/12bd4b4a/attachment.html>

From jasone at canonware.com  Thu Mar 17 10:45:32 2016
From: jasone at canonware.com (Jason Evans)
Date: Thu, 17 Mar 2016 10:45:32 -0700
Subject: Invalid conf pair
In-Reply-To: <CALodpJ1JAnoDCqKSq1M_QMBziL751H+7z5r=_jQafVDWeV-onQ@mail.gmail.com>
References: <CALodpJ1JAnoDCqKSq1M_QMBziL751H+7z5r=_jQafVDWeV-onQ@mail.gmail.com>
Message-ID: <594F53F0-0466-49D2-90D7-931DFAE16359@canonware.com>

On Mar 17, 2016, at 9:17 AM, Jeffery Griffith <jeffery.griffith at gmail.com> wrote:
> My first attempt at trying to integrate jemalloc and I can't get out of the starting blocks :-)  From what I read, it seems that I should be able to get jemalloc into the JVM with LD_PRELOAD and MALLOC_CONF as used below. I took the examples straight from the wiki but I must be missing something incredibly basic. I built the .so straight from the download with no problems (version 4.0) and my JVM does seem to run with it with no problem (radically lower virtual size and i can see the .so is loaded with pmap), but I clearly don't know what the hell I'm doing with MALLOC_CONF.
> 
> jgriffith at mydevbox.c1.:~/JEMALLOC$ echo $LD_PRELOAD
> /usr/local/lib/libjemalloc.so
> jgriffith at mydevbox.c1.:~/JEMALLOC$ echo $MALLOC_CONF
> prof:true,lg_prof_interval:30,lg_prof_sample:17
> jgriffith at mydevbox.c1.:~/JEMALLOC$ java -jar allocmemory.jar 
> <jemalloc>: Invalid conf pair: prof:true
> <jemalloc>: Invalid conf pair: lg_prof_interval:30
> <jemalloc>: Invalid conf pair: lg_prof_sample:17
> [...]

It looks like you didn't specify --enable-prof to the configure script.

Jason


From linuxhippy at gmail.com  Fri Mar 18 02:12:26 2016
From: linuxhippy at gmail.com (Clemens Eisserer)
Date: Fri, 18 Mar 2016 10:12:26 +0100
Subject: Tuning-advise for soft-realtime systems
In-Reply-To: <2B39F4D4-0753-4B6C-9063-88A480C2A4DA@canonware.com>
References: <CAFvQSYSmuWSfPFT7wCrMS7VzjetB_DqptEMNjpN=J21X5W1B2w@mail.gmail.com>
	<2B39F4D4-0753-4B6C-9063-88A480C2A4DA@canonware.com>
Message-ID: <CAFvQSYSe_71U7PCrVPb6kb5GyO9kjnpNwcdWrMvLXs-DhFcfFw@mail.gmail.com>

Hi Jason,

Thank you for your suggestions. Disabling dirty page purging helped a
lot, as did disabling transparent huge pages.
Worst-case latency for our test is now down to ~140?s compared to
glibc with ~100?s.

Am I right that the only syscall left is now the nmap for requesting new chunks?
Is there any way to preallocate the memory chunks used by jemalloc -
as there will be only one large-footpint application on the system, I
could afford to allocate e.g. 75% for this single high-priority
application.

Thank you & best regards, Clemens






2016-03-15 19:27 GMT+01:00 Jason Evans <jasone at canonware.com>:
> On Mar 14, 2016, at 10:52 AM, Clemens Eisserer <linuxhippy at gmail.com> wrote:
>> I am currently evaluating jemalloc for a soft-realtime system where
>> worst-case latency is of concern.
>> When allocating areas of random size honoring the size-distribution
>> which is to be expected, usually latency is of no concern.
>> However, from time to time there are latency spikes of ~300
>> microseconds for a single malloc especially for mid-sized allocations
>> (e.g. 1024byte).
>>
>> My guess is those latency spikes arise when jemalloc requests
>> additional memory for it's arena's from the kernel.
>> Are there any tuning knobs to reduce the duration of those spikes and
>> instead increasing there frequency?
>
> The top suggestions I can think of are to disable unused dirty page purging, and to increase the chunk size to the largest size you can tolerate in terms of memory usage.
>
> Jason

