<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> Fragmentation with jemalloc
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20Fragmentation%20with%20jemalloc&In-Reply-To=%3CCAB6p_24wkAuuM0cnVv-qpjbWBryOfB3YUBjR6cv2cA8Xv_r%2B0w%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000574.html">
   <LINK REL="Next"  HREF="000576.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>Fragmentation with jemalloc</H1>
    <B>Vinay Y S</B> 
    <A HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20Fragmentation%20with%20jemalloc&In-Reply-To=%3CCAB6p_24wkAuuM0cnVv-qpjbWBryOfB3YUBjR6cv2cA8Xv_r%2B0w%40mail.gmail.com%3E"
       TITLE="Fragmentation with jemalloc">vinay.ys at gmail.com
       </A><BR>
    <I>Mon Apr 22 22:37:48 PDT 2013</I>
    <P><UL>
        <LI>Previous message: <A HREF="000574.html">Fragmentation with jemalloc
</A></li>
        <LI>Next message: <A HREF="000576.html">Fragmentation with jemalloc
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#575">[ date ]</a>
              <a href="thread.html#575">[ thread ]</a>
              <a href="subject.html#575">[ subject ]</a>
              <a href="author.html#575">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Can Linux transparent huge pages cause this?


On Mon, Apr 22, 2013 at 10:34 PM, Jason Evans &lt;<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">jasone at canonware.com</A>&gt; wrote:

&gt;<i> On Apr 22, 2013, at 9:18 PM, vandana shah &lt;<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">shah.vandana at gmail.com</A>&gt; wrote:
</I>&gt;<i>
</I>&gt;<i> On Mon, Apr 22, 2013 at 11:49 PM, Jason Evans &lt;<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">jasone at canonware.com</A>&gt;wrote:
</I>&gt;<i>
</I>&gt;&gt;<i> On Apr 21, 2013, at 10:01 PM, vandana shah wrote:
</I>&gt;&gt;<i> &gt; I have been trying to use jemalloc for my application and observed that
</I>&gt;&gt;<i> the rss of the process keeps on increasing.
</I>&gt;&gt;<i> &gt;
</I>&gt;&gt;<i> &gt; I ran the application with valgrind to confirm that there are no memory
</I>&gt;&gt;<i> leaks.
</I>&gt;&gt;<i> &gt;
</I>&gt;&gt;<i> &gt; To investigate more, I collected jemalloc stats after running the test
</I>&gt;&gt;<i> for few days and here is the summary for a run with narenas:1,
</I>&gt;&gt;<i> tcache:false, lg_chunk:24
</I>&gt;&gt;<i> &gt;
</I>&gt;&gt;<i> &gt;  Arenas: 1
</I>&gt;&gt;<i> &gt;  Pointer size: 8
</I>&gt;&gt;<i> &gt;  Quantum size: 16
</I>&gt;&gt;<i> &gt;  Page size: 4096
</I>&gt;&gt;<i> &gt;  Min active:dirty page ratio per arena: 8:1
</I>&gt;&gt;<i> &gt;  Maximum thread-cached size class: 32768
</I>&gt;&gt;<i> &gt;  Chunk size: 16777216 (2^24)
</I>&gt;&gt;<i> &gt;  Allocated: 24364176040, active: 24578334720, mapped: 66739765248
</I>&gt;&gt;<i> &gt;  Current active ceiling: 24578621440
</I>&gt;&gt;<i> &gt;  chunks: nchunks   highchunks    curchunks
</I>&gt;&gt;<i> &gt;             3989         3978         3978
</I>&gt;&gt;<i> &gt;  huge: nmalloc      ndalloc    allocated
</I>&gt;&gt;<i> &gt;              3            2    117440512
</I>&gt;&gt;<i> &gt;
</I>&gt;&gt;<i> &gt;  arenas[0]:
</I>&gt;&gt;<i> &gt;  assigned threads: 17
</I>&gt;&gt;<i> &gt;  dss allocation precedence: disabled
</I>&gt;&gt;<i> &gt;  dirty pages: 5971898:64886 active:dirty, 354265 sweeps, 18261119
</I>&gt;&gt;<i> madvises, 1180858954 purged
</I>&gt;&gt;<i> &gt;
</I>&gt;&gt;<i> &gt; While in this state, the RSS of the process was at 54GB.
</I>&gt;&gt;<i> &gt;
</I>&gt;&gt;<i> &gt; Questions:
</I>&gt;&gt;<i> &gt; 1) The difference between RSS and jemalloc active is huge (more than
</I>&gt;&gt;<i> 30GB). In my test, the difference was quite less in the beginning (say 4
</I>&gt;&gt;<i> GB) and it went on increasing with time. That seems too high to account for
</I>&gt;&gt;<i> jemalloc data structures, overhead etc. What else gets accounted in process
</I>&gt;&gt;<i> RSS - active?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> jemalloc is reporting very low page-level external fragmentation for your
</I>&gt;&gt;<i> app: 1.0 - allocated/active == 1.0 - 24364176040/24578334720 == 0.87%.
</I>&gt;&gt;<i>  However, virtual memory fragmentation is quite high: 1.0 - active/mapped
</I>&gt;&gt;<i> == 63.2%.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> &gt; 2) The allocations are fairly random, sized between 8 bytes and 2MB.
</I>&gt;&gt;<i> Are there any known issues of fragmentation for particular allocation sizes?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> If your application were to commonly allocate slightly more than one
</I>&gt;&gt;<i> chunk, then internal fragmentation would be quite high, but at little
</I>&gt;&gt;<i> actual cost to physical memory.  However, you are using 16 MiB chunks, and
</I>&gt;&gt;<i> the stats say that there's only a single huge (112-MiB) allocation.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> &gt; 3) Is there a way to tune the allocations and reduce the difference?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I can't think of a way this could happen short of a bug in jemalloc.  Can
</I>&gt;&gt;<i> you send me a complete statistics, and provide the following?
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> - jemalloc version
</I>&gt;&gt;<i> - operating system
</I>&gt;&gt;<i> - compile-time jemalloc configuration flags
</I>&gt;&gt;<i> - run-time jemalloc option flags
</I>&gt;&gt;<i> - brief description of what application does
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Hopefully that will narrow down the possible explanations.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Thanks,
</I>&gt;&gt;<i> Jason
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Jemalloc version: 3.2.0
</I>&gt;<i> Operating system: Linux 2.6.32-220.7.1.el6.x86_64
</I>&gt;<i> Compile-time jemalloc configuration flags:
</I>&gt;<i> autogen            : 0
</I>&gt;<i> experimental       : 1
</I>&gt;<i> cc-silence         : 0
</I>&gt;<i> debug              : 0
</I>&gt;<i> stats              : 1
</I>&gt;<i> prof               : 0
</I>&gt;<i> prof-libunwind     : 0
</I>&gt;<i> prof-libgcc        : 0
</I>&gt;<i> prof-gcc           : 0
</I>&gt;<i> tcache             : 1
</I>&gt;<i> fill               : 1
</I>&gt;<i> utrace             : 0
</I>&gt;<i> valgrind           : 0
</I>&gt;<i> xmalloc            : 0
</I>&gt;<i> mremap             : 0
</I>&gt;<i> munmap             : 0
</I>&gt;<i> dss                : 0
</I>&gt;<i> lazy_lock          : 0
</I>&gt;<i> tls                : 1
</I>&gt;<i>
</I>&gt;<i> Run-time jemalloc configuration flags:
</I>&gt;<i> MALLOC_CONF=narenas:1,tcache:false,lg_chunk:24
</I>&gt;<i>
</I>&gt;<i> Application description:
</I>&gt;<i> This is a server that caches and serves data from sqlite database. The
</I>&gt;<i> database size can be multiple of the cache size.
</I>&gt;<i> The data is paged in and out as necessary to keep the process RSS under
</I>&gt;<i> control. The server is written in C++.
</I>&gt;<i> All data and metadata is dynamically allocated, so allocator is used quite
</I>&gt;<i> extensively.
</I>&gt;<i> In the test, server starts with a healthy data/RSS ratio (say 0.84). This
</I>&gt;<i> ratio reduces with time as RSS keeps growing
</I>&gt;<i> whereas server starts to page out data to keep RSS under control. In the
</I>&gt;<i> test the ratio came down to 0.42.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Okay, I've taken a close look at this, and I see no direct evidence of a
</I>&gt;<i> bug in jemalloc.  The difference between active and mapped memory is due to
</I>&gt;<i> page run fragmentation within the chunks, but the total
</I>&gt;<i> fragmentation-induced overhead attributable to chunk metadata and unused
</I>&gt;<i> dirty pages appears to be 200-300 MiB.  The only way I can see for the
</I>&gt;<i> statistics to be self-consistent, yet have such a high RSS is if the
</I>&gt;<i> madvise() call within pages_purge() is failing.  You should be able to
</I>&gt;<i> eliminate this possibility by looking at strace output.
</I>&gt;<i>
</I>&gt;<i> Are you certain that you are looking at RES (resident set size, aka RSS)
</I>&gt;<i> rather than VIRT (virtual size, aka VSIZE or VSZ)?  Assuming that your
</I>&gt;<i> application doesn't do a bunch of mmap()ing outside jemalloc, I would
</I>&gt;<i> expect VIRT to be pretty close to jemalloc's 'mapped' statistic, and RES to
</I>&gt;<i> be pretty close to jemalloc's 'active' statistic.
</I>&gt;<i>
</I>&gt;<i> Thanks,
</I>&gt;<i> Jason
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> _______________________________________________
</I>&gt;<i> jemalloc-discuss mailing list
</I>&gt;<i> <A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">jemalloc-discuss at canonware.com</A>
</I>&gt;<i> <A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">http://www.canonware.com/mailman/listinfo/jemalloc-discuss</A>
</I>&gt;<i>
</I>&gt;<i>
</I>-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130422/72ac7dd6/attachment.html">http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130422/72ac7dd6/attachment.html</A>&gt;
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000574.html">Fragmentation with jemalloc
</A></li>
	<LI>Next message: <A HREF="000576.html">Fragmentation with jemalloc
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#575">[ date ]</a>
              <a href="thread.html#575">[ thread ]</a>
              <a href="subject.html#575">[ subject ]</a>
              <a href="author.html#575">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">More information about the jemalloc-discuss
mailing list</a><br>
</body></html>
