<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> Transparent Huge Pages
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20Transparent%20Huge%20Pages&In-Reply-To=%3C4F437CA6.9000905%40cern.ch%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000081.html">
   <LINK REL="Next"  HREF="000083.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>Transparent Huge Pages</H1>
    <B>Jakob Blomer</B> 
    <A HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20Transparent%20Huge%20Pages&In-Reply-To=%3C4F437CA6.9000905%40cern.ch%3E"
       TITLE="Transparent Huge Pages">jakob.blomer at cern.ch
       </A><BR>
    <I>Tue Feb 21 03:14:46 PST 2012</I>
    <P><UL>
        <LI>Previous message: <A HREF="000081.html">Transparent Huge Pages
</A></li>
        <LI>Next message: <A HREF="000083.html">[PATCH] Remove unused variables in tcache_dalloc_large
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#82">[ date ]</a>
              <a href="thread.html#82">[ thread ]</a>
              <a href="subject.html#82">[ subject ]</a>
              <a href="author.html#82">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>You're right, the MADV_DONTNEED trick does work for the test program. 
If I understand correctly, the way it works is to first provoke a page 
fault to grab a 2M page, and then, by marking all but the first one of 
these 4k pages with MADV_DONTNEED, let the kernel split it into 4k pages 
and release the real memory.

The problem here is that after some time khugepaged kicks in and merges 
all the small pages back together into a large page.  That might be 
avoidable with another call to madvise() with MADV_NOHUGEPAGE (not in 
the RHEL 6.2 kernel, as far as I can see).  But even then, it looks to 
me a bit shaky.  Is there a guarantee that the kernel follows a 
MADV_DONTNEED advise?

I think for the moment I will check if in practice it just turns out to 
be a constant overhead of ~15M.  And in this case, live with it...

Cheers,
Jakob

On 2/20/12 11:08 PM, Justin Lebar wrote:
&gt;<i> Okay, this behavior is not entirely ridiculous, but at least Firefox's
</I>&gt;<i> fork of jemalloc will need to change to work well with this.
</I>&gt;<i>
</I>&gt;<i> What happens if you MADV_DONTNEED all but the first 4k after you touch
</I>&gt;<i> the first byte?  What about if you MADV_DONTNEED the whole thing
</I>&gt;<i> before you touch any part?
</I>&gt;<i>
</I>&gt;<i> On Mon, Feb 20, 2012 at 7:55 PM, Jakob Blomer&lt;<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">jakob.blomer at cern.ch</A>&gt;  wrote:
</I>&gt;&gt;<i> After thinking a bit more about it, I don't think it's a bug but this is
</I>&gt;&gt;<i> just the way transparent huge pages work.  For properly aligned memory, the
</I>&gt;&gt;<i> kernel takes a 2M page.  This just means 2M of real memory are gone, and I
</I>&gt;&gt;<i> think not even splitting afterwards can change that.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> The following program requires 300-400k RSS without transparent huge pages,
</I>&gt;&gt;<i> but&gt;2M with THP.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> #include&lt;unistd.h&gt;
</I>&gt;&gt;<i> #include&lt;sys/mman.h&gt;
</I>&gt;&gt;<i> #include&lt;stdio.h&gt;
</I>&gt;&gt;<i> #include&lt;errno.h&gt;
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> int main() {
</I>&gt;&gt;<i>   int size = 4*1024*1024;
</I>&gt;&gt;<i>   int _2m = 2*1024*1024;
</I>&gt;&gt;<i>   char *mapping = mmap(0x42000000, size, PROT_READ | PROT_WRITE,
</I>&gt;&gt;<i>                        MAP_ANONYMOUS | MAP_PRIVATE, -1, 0);
</I>&gt;&gt;<i>   mapping[0] = '\0';
</I>&gt;&gt;<i>   printf(&quot;Region of size %d mapped at %p (error %d), aligned at 2M: %d\n&quot;,
</I>&gt;&gt;<i> size, mapping, errno, (long)mapping%_2m);
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>   sleep(30);
</I>&gt;&gt;<i>   return 0;
</I>&gt;&gt;<i> }
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Cheers,
</I>&gt;&gt;<i> Jakob
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> On 2/20/12 5:10 PM, Justin Lebar wrote:
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Hm, upon further consideration...
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> If you mmap a huge page (say, 1MB), then MADV_DONTNEED a few 4-KB
</I>&gt;&gt;&gt;<i> chunks inside, transparent huge pages should break up the huge page so
</I>&gt;&gt;&gt;<i> it can decommit the parts I asked it to decommit.  If it doesn't, that
</I>&gt;&gt;&gt;<i> sounds like a kernel bug to me!
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Similarly, if I mmap 1MB, get a huge page, and then touch only a few
</I>&gt;&gt;&gt;<i> bytes in the middle, the kernel shouldn't commit a huge page.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> If huge pages is behaving how I expect, I don't see why it would cause
</I>&gt;&gt;&gt;<i> your application to use more memory.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Just to check, you're measuring RSS, not vsize, right?
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> On Mon, Feb 20, 2012 at 4:59 PM, Justin Lebar&lt;<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">justin.lebar at gmail.com</A>&gt;
</I>&gt;&gt;&gt;<i>   wrote:
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> jemalloc seems to be prone to transparent huge pages
</I>&gt;&gt;&gt;&gt;&gt;<i> (<A HREF="https://lwn.net/Articles/423584">https://lwn.net/Articles/423584</A>), presumably due to its use of mmap().
</I>&gt;&gt;&gt;&gt;&gt;<i>   In
</I>&gt;&gt;&gt;&gt;&gt;<i> my case (fuse module), the initial memory consumption jumped from ~12M
</I>&gt;&gt;&gt;&gt;&gt;<i> to
</I>&gt;&gt;&gt;&gt;&gt;<i> ~27M.  The use of --enable-dss helps a little, bringing the consumption
</I>&gt;&gt;&gt;&gt;&gt;<i> down
</I>&gt;&gt;&gt;&gt;&gt;<i> to ~19M.
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> Ouch!
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> Did anyone else experienced similar behavior?  Is there an easy way of
</I>&gt;&gt;&gt;&gt;&gt;<i> avoiding transparent huge pages for jemalloc'ed memory?  The only
</I>&gt;&gt;&gt;&gt;&gt;<i> workaround
</I>&gt;&gt;&gt;&gt;&gt;<i> that comes to my mind is a malloc wrapper that runs madvise(...,
</I>&gt;&gt;&gt;&gt;&gt;<i> MADV_NOHUGEPAGE) on every newly allocated chunk.
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;<i> You'd probably want to do this only on the 1MB chunks jemalloc
</I>&gt;&gt;&gt;&gt;<i> allocates for small and tiny allocations.  For huge allocations (more
</I>&gt;&gt;&gt;&gt;<i> than 1MB), it's likely the user will touch the whole thing, so huge
</I>&gt;&gt;&gt;&gt;<i> pages could be a benefit.
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> Cheers,
</I>&gt;&gt;&gt;&gt;&gt;<i> Jakob
</I>&gt;&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;&gt;&gt;<i> _______________________________________________
</I>&gt;&gt;&gt;&gt;&gt;<i> jemalloc-discuss mailing list
</I>&gt;&gt;&gt;&gt;&gt;<i> <A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">jemalloc-discuss at canonware.com</A>
</I>&gt;&gt;&gt;&gt;&gt;<i> <A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">http://www.canonware.com/mailman/listinfo/jemalloc-discuss</A>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> .
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;<i> .
</I>&gt;<i>
</I>

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000081.html">Transparent Huge Pages
</A></li>
	<LI>Next message: <A HREF="000083.html">[PATCH] Remove unused variables in tcache_dalloc_large
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#82">[ date ]</a>
              <a href="thread.html#82">[ thread ]</a>
              <a href="subject.html#82">[ subject ]</a>
              <a href="author.html#82">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">More information about the jemalloc-discuss
mailing list</a><br>
</body></html>
