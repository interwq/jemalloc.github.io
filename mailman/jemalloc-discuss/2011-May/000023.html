<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> New Allocator features
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20New%20Allocator%20features&In-Reply-To=%3C5afa08a1f923e85f240a4ce2de3127b0%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000022.html">
   <LINK REL="Next"  HREF="000024.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>New Allocator features</H1>
    <B>Terrell Magee</B> 
    <A HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20New%20Allocator%20features&In-Reply-To=%3C5afa08a1f923e85f240a4ce2de3127b0%40mail.gmail.com%3E"
       TITLE="New Allocator features">tmagee at conveycomputer.com
       </A><BR>
    <I>Thu May 12 06:33:49 PDT 2011</I>
    <P><UL>
        <LI>Previous message: <A HREF="000022.html">New Allocator features
</A></li>
        <LI>Next message: <A HREF="000024.html">64bit Literals
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#23">[ date ]</a>
              <a href="thread.html#23">[ thread ]</a>
              <a href="subject.html#23">[ subject ]</a>
              <a href="author.html#23">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>The system architecture is a basic NUMA memory system to begin with.
However, not only is access latency non-uniform, the ability to access all
nodes does not exist.  One of the nodes contains a coprocessor that plays
in the global virtual space of a process but requires special APIs for the
Host processor to access memory on the coprocessor node.  So while
technically it is possible to access the allocated buffer data, the
performance of doing so for management purposes makes it undesirable.

-----Original Message-----
From: Jason Evans [mailto:<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">jasone at canonware.com</A>]
Sent: Tuesday, May 10, 2011 11:44 PM
To: Terrell Magee
Cc: <A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">jemalloc-discuss at canonware.com</A>
Subject: Re: New Allocator features

On 05/10/2011 09:58 AM, Terrell Magee wrote:
&gt;<i> I have been using jemalloc for the past year with good results.  Our
</I>&gt;<i> product has a couple of new requirements that I have not come across
</I>before.
&gt;<i>
</I>&gt;<i> 1)The first is the ability to completely segregate control data from
</I>&gt;<i> allocated data.  This makes the allocator more like a resource manager
</I>&gt;<i> in that you allocate and mange the resource but have no access to it.
</I>&gt;<i> The data structure design of jemalloc lends itself fairly well to
</I>&gt;<i> meeting this requirement.
</I>
jemalloc takes advantage of constant-time address masking operations to
find metadata associated with allocated objects, so although it mostly
segregates metadata and data, completely abstracting the two away from
each other would take quite a bit of refactoring, not to mention that it
would probably slow down the allocator.  Backing up a bit, I'm trying to
imagine an allocator with this constraint, and it's immediately clear that
some standard features, like zero-filled memory via calloc(), would not be
possible.  What do you need such strong separation for?

&gt;<i> 2)The second requirement is focused on multi-node NUMA systems.
</I>&gt;<i> Consider the requirement to allocate memory on a specified node.  A
</I>&gt;<i> buffer is returned to the application on the requested node but the
</I>&gt;<i> application then migrates the data to another node for processing.
</I>&gt;<i> When the app frees the buffer, it is returned to the allocator's free
</I>pool.
&gt;<i> The problem is the memory is now located on the wrong node.
</I>&gt;<i> Subsequent allocations can result in misplaced data and performance
</I>anomalies.
&gt;<i> This behavior is true for any allocator; libc malloc(3), etc.
</I>
If you disable thread caches in jemalloc, and you're careful about your
thread--&gt;arena associations, you will be able to avoid the problem.
That is, memory is freed back to the arena from which it came, so as long
as each arena is only ever used for allocation on a single node (and the
allocated pages are touched before objects are passed to other threads),
all of that arena's memory will be locally attached.

As an aside, I'm planning to experiment with using sched_getcpu(3) on
Linux to choose the arena to allocate from.  Right now there are 4*ncpus
arenas by default, and threads are uniformly distributed among the arenas
in order to reduce lock contention.  With sched_getcpu(3) we should be
able to effectively use only 1*ncpus arenas.  Furthermore, the application
won't have to mess around with thread--&gt;arena associations; instead it can
set thread--&gt;CPU affinity as desired, and jemalloc will automatically work
well.

Cheers,
Jason

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000022.html">New Allocator features
</A></li>
	<LI>Next message: <A HREF="000024.html">64bit Literals
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#23">[ date ]</a>
              <a href="thread.html#23">[ thread ]</a>
              <a href="subject.html#23">[ subject ]</a>
              <a href="author.html#23">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">More information about the jemalloc-discuss
mailing list</a><br>
</body></html>
