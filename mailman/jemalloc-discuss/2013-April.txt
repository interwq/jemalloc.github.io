From shah.vandana at gmail.com  Sun Apr 21 22:01:49 2013
From: shah.vandana at gmail.com (vandana shah)
Date: Mon, 22 Apr 2013 10:31:49 +0530
Subject: Fragmentation with jemalloc
Message-ID: <CAKtxisVw5PAV2kCb8QyPCUSC58gK31Qn=RTGKOcT2wvZPMKL9w@mail.gmail.com>

Hi,

I have been trying to use jemalloc for my application and observed that the
rss of the process keeps on increasing.

I ran the application with valgrind to confirm that there are no memory
leaks.

To investigate more, I collected jemalloc stats after running the test for
few days and here is the summary for a run with narenas:1, tcache:false,
lg_chunk:24

 Arenas: 1
 Pointer size: 8
 Quantum size: 16
 Page size: 4096
 Min active:dirty page ratio per arena: 8:1
 Maximum thread-cached size class: 32768
 Chunk size: 16777216 (2^24)
 Allocated: 24364176040, active: 24578334720, mapped: 66739765248
 Current active ceiling: 24578621440
 chunks: nchunks   highchunks    curchunks
            3989         3978         3978
 huge: nmalloc      ndalloc    allocated
             3            2    117440512

 arenas[0]:
 assigned threads: 17
 dss allocation precedence: disabled
 dirty pages: 5971898:64886 active:dirty, 354265 sweeps, 18261119 madvises,
1180858954 purged

While in this state, the RSS of the process was at 54GB.

Questions:
1) The difference between RSS and jemalloc active is huge (more than 30GB).
In my test, the difference was quite less in the beginning (say 4 GB) and
it went on increasing with time. That seems too high to account for
jemalloc data structures, overhead etc. What else gets accounted in process
RSS - active?
2) The allocations are fairly random, sized between 8 bytes and 2MB. Are
there any known issues of fragmentation for particular allocation sizes?
3) Is there a way to tune the allocations and reduce the difference?

Will appreciate any help to understand this better,

Thanks,
Vandana
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130422/774f20c4/attachment.html>

From jasone at canonware.com  Mon Apr 22 11:19:05 2013
From: jasone at canonware.com (Jason Evans)
Date: Mon, 22 Apr 2013 11:19:05 -0700
Subject: Fragmentation with jemalloc
In-Reply-To: <CAKtxisVw5PAV2kCb8QyPCUSC58gK31Qn=RTGKOcT2wvZPMKL9w@mail.gmail.com>
References: <CAKtxisVw5PAV2kCb8QyPCUSC58gK31Qn=RTGKOcT2wvZPMKL9w@mail.gmail.com>
Message-ID: <01C00994-F997-4C63-B3F9-4915843F1937@canonware.com>

On Apr 21, 2013, at 10:01 PM, vandana shah wrote:
> I have been trying to use jemalloc for my application and observed that the rss of the process keeps on increasing.
> 
> I ran the application with valgrind to confirm that there are no memory leaks.
> 
> To investigate more, I collected jemalloc stats after running the test for few days and here is the summary for a run with narenas:1, tcache:false, lg_chunk:24
> 
>  Arenas: 1
>  Pointer size: 8
>  Quantum size: 16
>  Page size: 4096
>  Min active:dirty page ratio per arena: 8:1
>  Maximum thread-cached size class: 32768
>  Chunk size: 16777216 (2^24)
>  Allocated: 24364176040, active: 24578334720, mapped: 66739765248
>  Current active ceiling: 24578621440
>  chunks: nchunks   highchunks    curchunks
>             3989         3978         3978
>  huge: nmalloc      ndalloc    allocated
>              3            2    117440512
>  
>  arenas[0]:
>  assigned threads: 17
>  dss allocation precedence: disabled
>  dirty pages: 5971898:64886 active:dirty, 354265 sweeps, 18261119 madvises, 1180858954 purged
> 
> While in this state, the RSS of the process was at 54GB.
> 
> Questions:
> 1) The difference between RSS and jemalloc active is huge (more than 30GB). In my test, the difference was quite less in the beginning (say 4 GB) and it went on increasing with time. That seems too high to account for jemalloc data structures, overhead etc. What else gets accounted in process RSS - active?

jemalloc is reporting very low page-level external fragmentation for your app: 1.0 - allocated/active == 1.0 - 24364176040/24578334720 == 0.87%.  However, virtual memory fragmentation is quite high: 1.0 - active/mapped == 63.2%.

> 2) The allocations are fairly random, sized between 8 bytes and 2MB. Are there any known issues of fragmentation for particular allocation sizes?

If your application were to commonly allocate slightly more than one chunk, then internal fragmentation would be quite high, but at little actual cost to physical memory.  However, you are using 16 MiB chunks, and the stats say that there's only a single huge (112-MiB) allocation.

> 3) Is there a way to tune the allocations and reduce the difference?

I can't think of a way this could happen short of a bug in jemalloc.  Can you send me a complete statistics, and provide the following?

- jemalloc version
- operating system
- compile-time jemalloc configuration flags
- run-time jemalloc option flags
- brief description of what application does

Hopefully that will narrow down the possible explanations.

Thanks,
Jason

From shah.vandana at gmail.com  Mon Apr 22 21:25:54 2013
From: shah.vandana at gmail.com (vandana shah)
Date: Tue, 23 Apr 2013 09:55:54 +0530
Subject: Fragmentation with jemalloc
In-Reply-To: <01C00994-F997-4C63-B3F9-4915843F1937@canonware.com>
References: <CAKtxisVw5PAV2kCb8QyPCUSC58gK31Qn=RTGKOcT2wvZPMKL9w@mail.gmail.com>
	<01C00994-F997-4C63-B3F9-4915843F1937@canonware.com>
Message-ID: <CAKtxisUkeZe-ninM1E3xXsCT2RWV1yNaGC3wHvFT_aReYZjzJA@mail.gmail.com>

Jemalloc version: 3.2.0
Operating system: Linux 2.6.32-220.7.1.el6.x86_64
Compile-time jemalloc configuration flags:
autogen            : 0
experimental       : 1
cc-silence         : 0
debug              : 0
stats              : 1
prof               : 0
prof-libunwind     : 0
prof-libgcc        : 0
prof-gcc           : 0
tcache             : 1
fill               : 1
utrace             : 0
valgrind           : 0
xmalloc            : 0
mremap             : 0
munmap             : 0
dss                : 0
lazy_lock          : 0
tls                : 1

Run-time jemalloc configuration flags:
MALLOC_CONF=narenas:1,tcache:false,lg_chunk:24

Application description:
This is a server that caches and serves data from sqlite database. The
database size can be multiple of the cache size.
The data is paged in and out as necessary to keep the process RSS under
control. The server is written in C++.
All data and metadata is dynamically allocated, so allocator is used quite
extensively.
In the test, server starts with a healthy data/RSS ratio (say 0.84). This
ratio reduces with time as RSS keeps growing
whereas server starts to page out data to keep RSS under control. In the
test the ratio came down to 0.42.

thanks,
Vandana


On Mon, Apr 22, 2013 at 11:49 PM, Jason Evans <jasone at canonware.com> wrote:

> On Apr 21, 2013, at 10:01 PM, vandana shah wrote:
> > I have been trying to use jemalloc for my application and observed that
> the rss of the process keeps on increasing.
> >
> > I ran the application with valgrind to confirm that there are no memory
> leaks.
> >
> > To investigate more, I collected jemalloc stats after running the test
> for few days and here is the summary for a run with narenas:1,
> tcache:false, lg_chunk:24
> >
> >  Arenas: 1
> >  Pointer size: 8
> >  Quantum size: 16
> >  Page size: 4096
> >  Min active:dirty page ratio per arena: 8:1
> >  Maximum thread-cached size class: 32768
> >  Chunk size: 16777216 (2^24)
> >  Allocated: 24364176040, active: 24578334720, mapped: 66739765248
> >  Current active ceiling: 24578621440
> >  chunks: nchunks   highchunks    curchunks
> >             3989         3978         3978
> >  huge: nmalloc      ndalloc    allocated
> >              3            2    117440512
> >
> >  arenas[0]:
> >  assigned threads: 17
> >  dss allocation precedence: disabled
> >  dirty pages: 5971898:64886 active:dirty, 354265 sweeps, 18261119
> madvises, 1180858954 purged
> >
> > While in this state, the RSS of the process was at 54GB.
> >
> > Questions:
> > 1) The difference between RSS and jemalloc active is huge (more than
> 30GB). In my test, the difference was quite less in the beginning (say 4
> GB) and it went on increasing with time. That seems too high to account for
> jemalloc data structures, overhead etc. What else gets accounted in process
> RSS - active?
>
> jemalloc is reporting very low page-level external fragmentation for your
> app: 1.0 - allocated/active == 1.0 - 24364176040/24578334720 == 0.87%.
>  However, virtual memory fragmentation is quite high: 1.0 - active/mapped
> == 63.2%.
>
> > 2) The allocations are fairly random, sized between 8 bytes and 2MB. Are
> there any known issues of fragmentation for particular allocation sizes?
>
> If your application were to commonly allocate slightly more than one
> chunk, then internal fragmentation would be quite high, but at little
> actual cost to physical memory.  However, you are using 16 MiB chunks, and
> the stats say that there's only a single huge (112-MiB) allocation.
>
> > 3) Is there a way to tune the allocations and reduce the difference?
>
> I can't think of a way this could happen short of a bug in jemalloc.  Can
> you send me a complete statistics, and provide the following?
>
> - jemalloc version
> - operating system
> - compile-time jemalloc configuration flags
> - run-time jemalloc option flags
> - brief description of what application does
>
> Hopefully that will narrow down the possible explanations.
>
> Thanks,
> Jason
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130423/c221f43c/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: bad.log.gz
Type: application/x-gzip
Size: 8167 bytes
Desc: not available
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130423/c221f43c/attachment.bin>

From jasone at canonware.com  Mon Apr 22 22:34:36 2013
From: jasone at canonware.com (Jason Evans)
Date: Mon, 22 Apr 2013 22:34:36 -0700
Subject: Fragmentation with jemalloc
In-Reply-To: <CAKtxisVV_9SJOvMUgcM6Q9p0Tdvn+USJBvSatCjuFE58+DBx6Q@mail.gmail.com>
References: <CAKtxisVw5PAV2kCb8QyPCUSC58gK31Qn=RTGKOcT2wvZPMKL9w@mail.gmail.com>
	<01C00994-F997-4C63-B3F9-4915843F1937@canonware.com>
	<CAKtxisVV_9SJOvMUgcM6Q9p0Tdvn+USJBvSatCjuFE58+DBx6Q@mail.gmail.com>
Message-ID: <228F3AC7-E7B3-48FB-84E6-2A7B31A20465@canonware.com>

On Apr 22, 2013, at 9:18 PM, vandana shah <shah.vandana at gmail.com> wrote:
> On Mon, Apr 22, 2013 at 11:49 PM, Jason Evans <jasone at canonware.com> wrote:
> On Apr 21, 2013, at 10:01 PM, vandana shah wrote:
> > I have been trying to use jemalloc for my application and observed that the rss of the process keeps on increasing.
> >
> > I ran the application with valgrind to confirm that there are no memory leaks.
> >
> > To investigate more, I collected jemalloc stats after running the test for few days and here is the summary for a run with narenas:1, tcache:false, lg_chunk:24
> >
> >  Arenas: 1
> >  Pointer size: 8
> >  Quantum size: 16
> >  Page size: 4096
> >  Min active:dirty page ratio per arena: 8:1
> >  Maximum thread-cached size class: 32768
> >  Chunk size: 16777216 (2^24)
> >  Allocated: 24364176040, active: 24578334720, mapped: 66739765248
> >  Current active ceiling: 24578621440
> >  chunks: nchunks   highchunks    curchunks
> >             3989         3978         3978
> >  huge: nmalloc      ndalloc    allocated
> >              3            2    117440512
> >
> >  arenas[0]:
> >  assigned threads: 17
> >  dss allocation precedence: disabled
> >  dirty pages: 5971898:64886 active:dirty, 354265 sweeps, 18261119 madvises, 1180858954 purged
> >
> > While in this state, the RSS of the process was at 54GB.
> >
> > Questions:
> > 1) The difference between RSS and jemalloc active is huge (more than 30GB). In my test, the difference was quite less in the beginning (say 4 GB) and it went on increasing with time. That seems too high to account for jemalloc data structures, overhead etc. What else gets accounted in process RSS - active?
> 
> jemalloc is reporting very low page-level external fragmentation for your app: 1.0 - allocated/active == 1.0 - 24364176040/24578334720 == 0.87%.  However, virtual memory fragmentation is quite high: 1.0 - active/mapped == 63.2%.
> 
> > 2) The allocations are fairly random, sized between 8 bytes and 2MB. Are there any known issues of fragmentation for particular allocation sizes?
> 
> If your application were to commonly allocate slightly more than one chunk, then internal fragmentation would be quite high, but at little actual cost to physical memory.  However, you are using 16 MiB chunks, and the stats say that there's only a single huge (112-MiB) allocation.
> 
> > 3) Is there a way to tune the allocations and reduce the difference?
> 
> I can't think of a way this could happen short of a bug in jemalloc.  Can you send me a complete statistics, and provide the following?
> 
> - jemalloc version
> - operating system
> - compile-time jemalloc configuration flags
> - run-time jemalloc option flags
> - brief description of what application does
> 
> Hopefully that will narrow down the possible explanations.
> 
> Thanks,
> Jason
> 
> Jemalloc version: 3.2.0
> Operating system: Linux 2.6.32-220.7.1.el6.x86_64
> Compile-time jemalloc configuration flags:
> autogen            : 0
> experimental       : 1
> cc-silence         : 0
> debug              : 0
> stats              : 1
> prof               : 0
> prof-libunwind     : 0
> prof-libgcc        : 0
> prof-gcc           : 0
> tcache             : 1
> fill               : 1
> utrace             : 0
> valgrind           : 0
> xmalloc            : 0
> mremap             : 0
> munmap             : 0
> dss                : 0
> lazy_lock          : 0
> tls                : 1
> 
> Run-time jemalloc configuration flags:
> MALLOC_CONF=narenas:1,tcache:false,lg_chunk:24
> 
> Application description:
> This is a server that caches and serves data from sqlite database. The database size can be multiple of the cache size.
> The data is paged in and out as necessary to keep the process RSS under control. The server is written in C++.
> All data and metadata is dynamically allocated, so allocator is used quite extensively.
> In the test, server starts with a healthy data/RSS ratio (say 0.84). This ratio reduces with time as RSS keeps growing
> whereas server starts to page out data to keep RSS under control. In the test the ratio came down to 0.42.

Okay, I've taken a close look at this, and I see no direct evidence of a bug in jemalloc.  The difference between active and mapped memory is due to page run fragmentation within the chunks, but the total fragmentation-induced overhead attributable to chunk metadata and unused dirty pages appears to be 200-300 MiB.  The only way I can see for the statistics to be self-consistent, yet have such a high RSS is if the madvise() call within pages_purge() is failing.  You should be able to eliminate this possibility by looking at strace output.

Are you certain that you are looking at RES (resident set size, aka RSS) rather than VIRT (virtual size, aka VSIZE or VSZ)?  Assuming that your application doesn't do a bunch of mmap()ing outside jemalloc, I would expect VIRT to be pretty close to jemalloc's 'mapped' statistic, and RES to be pretty close to jemalloc's 'active' statistic.

Thanks,
Jason

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130422/8c4edfb6/attachment.html>

From vinay.ys at gmail.com  Mon Apr 22 22:37:48 2013
From: vinay.ys at gmail.com (Vinay Y S)
Date: Mon, 22 Apr 2013 22:37:48 -0700
Subject: Fragmentation with jemalloc
In-Reply-To: <228F3AC7-E7B3-48FB-84E6-2A7B31A20465@canonware.com>
References: <CAKtxisVw5PAV2kCb8QyPCUSC58gK31Qn=RTGKOcT2wvZPMKL9w@mail.gmail.com>
	<01C00994-F997-4C63-B3F9-4915843F1937@canonware.com>
	<CAKtxisVV_9SJOvMUgcM6Q9p0Tdvn+USJBvSatCjuFE58+DBx6Q@mail.gmail.com>
	<228F3AC7-E7B3-48FB-84E6-2A7B31A20465@canonware.com>
Message-ID: <CAB6p_24wkAuuM0cnVv-qpjbWBryOfB3YUBjR6cv2cA8Xv_r+0w@mail.gmail.com>

Can Linux transparent huge pages cause this?


On Mon, Apr 22, 2013 at 10:34 PM, Jason Evans <jasone at canonware.com> wrote:

> On Apr 22, 2013, at 9:18 PM, vandana shah <shah.vandana at gmail.com> wrote:
>
> On Mon, Apr 22, 2013 at 11:49 PM, Jason Evans <jasone at canonware.com>wrote:
>
>> On Apr 21, 2013, at 10:01 PM, vandana shah wrote:
>> > I have been trying to use jemalloc for my application and observed that
>> the rss of the process keeps on increasing.
>> >
>> > I ran the application with valgrind to confirm that there are no memory
>> leaks.
>> >
>> > To investigate more, I collected jemalloc stats after running the test
>> for few days and here is the summary for a run with narenas:1,
>> tcache:false, lg_chunk:24
>> >
>> >  Arenas: 1
>> >  Pointer size: 8
>> >  Quantum size: 16
>> >  Page size: 4096
>> >  Min active:dirty page ratio per arena: 8:1
>> >  Maximum thread-cached size class: 32768
>> >  Chunk size: 16777216 (2^24)
>> >  Allocated: 24364176040, active: 24578334720, mapped: 66739765248
>> >  Current active ceiling: 24578621440
>> >  chunks: nchunks   highchunks    curchunks
>> >             3989         3978         3978
>> >  huge: nmalloc      ndalloc    allocated
>> >              3            2    117440512
>> >
>> >  arenas[0]:
>> >  assigned threads: 17
>> >  dss allocation precedence: disabled
>> >  dirty pages: 5971898:64886 active:dirty, 354265 sweeps, 18261119
>> madvises, 1180858954 purged
>> >
>> > While in this state, the RSS of the process was at 54GB.
>> >
>> > Questions:
>> > 1) The difference between RSS and jemalloc active is huge (more than
>> 30GB). In my test, the difference was quite less in the beginning (say 4
>> GB) and it went on increasing with time. That seems too high to account for
>> jemalloc data structures, overhead etc. What else gets accounted in process
>> RSS - active?
>>
>> jemalloc is reporting very low page-level external fragmentation for your
>> app: 1.0 - allocated/active == 1.0 - 24364176040/24578334720 == 0.87%.
>>  However, virtual memory fragmentation is quite high: 1.0 - active/mapped
>> == 63.2%.
>>
>> > 2) The allocations are fairly random, sized between 8 bytes and 2MB.
>> Are there any known issues of fragmentation for particular allocation sizes?
>>
>> If your application were to commonly allocate slightly more than one
>> chunk, then internal fragmentation would be quite high, but at little
>> actual cost to physical memory.  However, you are using 16 MiB chunks, and
>> the stats say that there's only a single huge (112-MiB) allocation.
>>
>> > 3) Is there a way to tune the allocations and reduce the difference?
>>
>> I can't think of a way this could happen short of a bug in jemalloc.  Can
>> you send me a complete statistics, and provide the following?
>>
>> - jemalloc version
>> - operating system
>> - compile-time jemalloc configuration flags
>> - run-time jemalloc option flags
>> - brief description of what application does
>>
>> Hopefully that will narrow down the possible explanations.
>>
>> Thanks,
>> Jason
>
>
> Jemalloc version: 3.2.0
> Operating system: Linux 2.6.32-220.7.1.el6.x86_64
> Compile-time jemalloc configuration flags:
> autogen            : 0
> experimental       : 1
> cc-silence         : 0
> debug              : 0
> stats              : 1
> prof               : 0
> prof-libunwind     : 0
> prof-libgcc        : 0
> prof-gcc           : 0
> tcache             : 1
> fill               : 1
> utrace             : 0
> valgrind           : 0
> xmalloc            : 0
> mremap             : 0
> munmap             : 0
> dss                : 0
> lazy_lock          : 0
> tls                : 1
>
> Run-time jemalloc configuration flags:
> MALLOC_CONF=narenas:1,tcache:false,lg_chunk:24
>
> Application description:
> This is a server that caches and serves data from sqlite database. The
> database size can be multiple of the cache size.
> The data is paged in and out as necessary to keep the process RSS under
> control. The server is written in C++.
> All data and metadata is dynamically allocated, so allocator is used quite
> extensively.
> In the test, server starts with a healthy data/RSS ratio (say 0.84). This
> ratio reduces with time as RSS keeps growing
> whereas server starts to page out data to keep RSS under control. In the
> test the ratio came down to 0.42.
>
>
> Okay, I've taken a close look at this, and I see no direct evidence of a
> bug in jemalloc.  The difference between active and mapped memory is due to
> page run fragmentation within the chunks, but the total
> fragmentation-induced overhead attributable to chunk metadata and unused
> dirty pages appears to be 200-300 MiB.  The only way I can see for the
> statistics to be self-consistent, yet have such a high RSS is if the
> madvise() call within pages_purge() is failing.  You should be able to
> eliminate this possibility by looking at strace output.
>
> Are you certain that you are looking at RES (resident set size, aka RSS)
> rather than VIRT (virtual size, aka VSIZE or VSZ)?  Assuming that your
> application doesn't do a bunch of mmap()ing outside jemalloc, I would
> expect VIRT to be pretty close to jemalloc's 'mapped' statistic, and RES to
> be pretty close to jemalloc's 'active' statistic.
>
> Thanks,
> Jason
>
>
> _______________________________________________
> jemalloc-discuss mailing list
> jemalloc-discuss at canonware.com
> http://www.canonware.com/mailman/listinfo/jemalloc-discuss
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130422/72ac7dd6/attachment.html>

From shah.vandana at gmail.com  Mon Apr 22 23:36:41 2013
From: shah.vandana at gmail.com (vandana shah)
Date: Tue, 23 Apr 2013 12:06:41 +0530
Subject: Fragmentation with jemalloc
In-Reply-To: <228F3AC7-E7B3-48FB-84E6-2A7B31A20465@canonware.com>
References: <CAKtxisVw5PAV2kCb8QyPCUSC58gK31Qn=RTGKOcT2wvZPMKL9w@mail.gmail.com>
	<01C00994-F997-4C63-B3F9-4915843F1937@canonware.com>
	<CAKtxisVV_9SJOvMUgcM6Q9p0Tdvn+USJBvSatCjuFE58+DBx6Q@mail.gmail.com>
	<228F3AC7-E7B3-48FB-84E6-2A7B31A20465@canonware.com>
Message-ID: <CAKtxisV+TKCCenoUx6YzNEk7E2TV7TVhjDNEkhA=FbxuBHXCxg@mail.gmail.com>

I am quite certain I am looking at RES and not VIRT. In the tests, VIRT
remains close to jemalloc's 'mapped' statistic, but resident set size is
way off 'active' reported by jemalloc.

I will check if madvise fails in the tests and get back.

Thanks,
Vandana


On Tue, Apr 23, 2013 at 11:04 AM, Jason Evans <jasone at canonware.com> wrote:

> On Apr 22, 2013, at 9:18 PM, vandana shah <shah.vandana at gmail.com> wrote:
>
> On Mon, Apr 22, 2013 at 11:49 PM, Jason Evans <jasone at canonware.com>wrote:
>
>> On Apr 21, 2013, at 10:01 PM, vandana shah wrote:
>> > I have been trying to use jemalloc for my application and observed that
>> the rss of the process keeps on increasing.
>> >
>> > I ran the application with valgrind to confirm that there are no memory
>> leaks.
>> >
>> > To investigate more, I collected jemalloc stats after running the test
>> for few days and here is the summary for a run with narenas:1,
>> tcache:false, lg_chunk:24
>> >
>> >  Arenas: 1
>> >  Pointer size: 8
>> >  Quantum size: 16
>> >  Page size: 4096
>> >  Min active:dirty page ratio per arena: 8:1
>> >  Maximum thread-cached size class: 32768
>> >  Chunk size: 16777216 (2^24)
>> >  Allocated: 24364176040, active: 24578334720, mapped: 66739765248
>> >  Current active ceiling: 24578621440
>> >  chunks: nchunks   highchunks    curchunks
>> >             3989         3978         3978
>> >  huge: nmalloc      ndalloc    allocated
>> >              3            2    117440512
>> >
>> >  arenas[0]:
>> >  assigned threads: 17
>> >  dss allocation precedence: disabled
>> >  dirty pages: 5971898:64886 active:dirty, 354265 sweeps, 18261119
>> madvises, 1180858954 purged
>> >
>> > While in this state, the RSS of the process was at 54GB.
>> >
>> > Questions:
>> > 1) The difference between RSS and jemalloc active is huge (more than
>> 30GB). In my test, the difference was quite less in the beginning (say 4
>> GB) and it went on increasing with time. That seems too high to account for
>> jemalloc data structures, overhead etc. What else gets accounted in process
>> RSS - active?
>>
>> jemalloc is reporting very low page-level external fragmentation for your
>> app: 1.0 - allocated/active == 1.0 - 24364176040/24578334720 == 0.87%.
>>  However, virtual memory fragmentation is quite high: 1.0 - active/mapped
>> == 63.2%.
>>
>> > 2) The allocations are fairly random, sized between 8 bytes and 2MB.
>> Are there any known issues of fragmentation for particular allocation sizes?
>>
>> If your application were to commonly allocate slightly more than one
>> chunk, then internal fragmentation would be quite high, but at little
>> actual cost to physical memory.  However, you are using 16 MiB chunks, and
>> the stats say that there's only a single huge (112-MiB) allocation.
>>
>> > 3) Is there a way to tune the allocations and reduce the difference?
>>
>> I can't think of a way this could happen short of a bug in jemalloc.  Can
>> you send me a complete statistics, and provide the following?
>>
>> - jemalloc version
>> - operating system
>> - compile-time jemalloc configuration flags
>> - run-time jemalloc option flags
>> - brief description of what application does
>>
>> Hopefully that will narrow down the possible explanations.
>>
>> Thanks,
>> Jason
>
>
> Jemalloc version: 3.2.0
> Operating system: Linux 2.6.32-220.7.1.el6.x86_64
> Compile-time jemalloc configuration flags:
> autogen            : 0
> experimental       : 1
> cc-silence         : 0
> debug              : 0
> stats              : 1
> prof               : 0
> prof-libunwind     : 0
> prof-libgcc        : 0
> prof-gcc           : 0
> tcache             : 1
> fill               : 1
> utrace             : 0
> valgrind           : 0
> xmalloc            : 0
> mremap             : 0
> munmap             : 0
> dss                : 0
> lazy_lock          : 0
> tls                : 1
>
> Run-time jemalloc configuration flags:
> MALLOC_CONF=narenas:1,tcache:false,lg_chunk:24
>
> Application description:
> This is a server that caches and serves data from sqlite database. The
> database size can be multiple of the cache size.
> The data is paged in and out as necessary to keep the process RSS under
> control. The server is written in C++.
> All data and metadata is dynamically allocated, so allocator is used quite
> extensively.
> In the test, server starts with a healthy data/RSS ratio (say 0.84). This
> ratio reduces with time as RSS keeps growing
> whereas server starts to page out data to keep RSS under control. In the
> test the ratio came down to 0.42.
>
>
> Okay, I've taken a close look at this, and I see no direct evidence of a
> bug in jemalloc.  The difference between active and mapped memory is due to
> page run fragmentation within the chunks, but the total
> fragmentation-induced overhead attributable to chunk metadata and unused
> dirty pages appears to be 200-300 MiB.  The only way I can see for the
> statistics to be self-consistent, yet have such a high RSS is if the
> madvise() call within pages_purge() is failing.  You should be able to
> eliminate this possibility by looking at strace output.
>
> Are you certain that you are looking at RES (resident set size, aka RSS)
> rather than VIRT (virtual size, aka VSIZE or VSZ)?  Assuming that your
> application doesn't do a bunch of mmap()ing outside jemalloc, I would
> expect VIRT to be pretty close to jemalloc's 'mapped' statistic, and RES to
> be pretty close to jemalloc's 'active' statistic.
>
> Thanks,
> Jason
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130423/00b94c3f/attachment.html>

From garg_rajat at hotmail.com  Fri Apr 26 09:39:10 2013
From: garg_rajat at hotmail.com (Rajat Garg)
Date: Fri, 26 Apr 2013 09:39:10 -0700
Subject: jemalloc performance for very small allocations
In-Reply-To: <BAY154-W8749FBE688DE4A8A678A3EFB70@phx.gbl>
References: <BAY154-W8749FBE688DE4A8A678A3EFB70@phx.gbl>
Message-ID: <BAY154-W185E683D345520A347B5EDEFB70@phx.gbl>


Hi,
 I am using jemalloc 3.1.0, compiled with gcc 4.1.2 on CentOS release 5.6 for a compute intensive multithreaded application where bulk of allocations falls in 8-byte, 16-byte (stats below). We are seeing very high runtime in tcache_alloc_small_hard(), arena_tcache_fill_small() and arena_bin_malloc_hard().  The 8 and 16 bytes allocations happen in very large number. We probably want the allocations to come form tcache_alloc_easy() to not hit the locks and take less runtime.  The allocations are mostly temporary in nature -- especially 8-byte ones -- so some number of 8-byte allocations are done, then they are freed and then alloc/free process repeated. 

The runs use 8 threads (so 48 arenas and all other jemalloc settings are default) and are on an Intel Xeon server with 12-cores (no hyperthreading and no other user so no contention with other user; the peak memory in application is ~6.5GB and server has 128GB memory so no memory shortage/swapping etc.); the jemalloc output at the end of run is given below. As we can see, pretty much 8 and 16 byte bins are overloaded. 

A run with TCACHE_NSLOTS_SMALL_MAX=10000, LG_TCACHE_MAXCLASS_DEFAULT=10, changing small bins to hold only upto 224 bytes (NBINS=15),  and hardcoding 
tcache_bin_info[i].ncached_max = TCACHE_NSLOTS_SMALL_MAX in tcache.c file improves runtime by about 16% (for overall application runtime) but increases peak memory from 6.5GB to 6.7GB. We want the runtime to at least improve by another 10% without preferably any increase in peak memory (so even 6.5GB to 6.7GB is not desirable). 
Any suggestions on what changes to jemalloc settings to try?
thanks a lot!,
--Rajat

___ Begin jemalloc statistics ___
Version: 3.1.0-0-g3b1f3aca54fede23299cde9034f7b909c3d290d7
Assertions disabled
Run-time option settings:
  opt.abort: false
  opt.lg_chunk: 22
  opt.dss: "secondary"
  opt.narenas: 48
  opt.lg_dirty_mult: 5
  opt.stats_print: false
  opt.junk: false
  opt.quarantine: 0
  opt.redzone: false
  opt.zero: false
  opt.tcache: true
  opt.lg_tcache_max: 15
CPUs: 12
Arenas: 48
Pointer size: 8
Quantum size: 16
Page size: 4096
Min active:dirty page ratio per arena: 32:1
Maximum thread-cached size class: 32768
Chunk size: 4194304 (2^22)
Allocated: 1209815024, active: 1295314944, mapped: 1832910848
Current active ceiling: 1321205760
chunks: nchunks   highchunks    curchunks
           446          443          437
huge: nmalloc      ndalloc    allocated
            0            0            0

Merged arenas stats:
assigned threads: 1
dss allocation precedence: N/A
dirty pages: 316239:9435 active:dirty, 1133 sweeps, 43514 madvises, 198766 purged
            allocated      nmalloc      ndalloc    nrequests
small:     1048219632  14499926892  14459296188  16587774427
large:      161595392         1658         1282         2659
total:     1209815024  14499928550  14459297470  16587777086
active:    1295314944
mapped:    1828716544
bins:     bin  size regs pgs    allocated      nmalloc      ndalloc    nrequests       nfills     nflushes      newruns       reruns      curruns
            0     8  501   1     17659680   7173034900   7170827440   8020202160     71750873     71709910       282770     17873119         4463
            1    16  252   1    399942864   7263860606   7238864177   8275464931     72760253     72395028       138537   1394609632       105755
            2    32  126   1    340674496     34841421     24195343    229801652       504480       257943       128506      1157771        89699
            3    48   84   1     78136368      2304192       676351     11567784       101000        18024        22085        18898        19409
            4    64   63   1      1573312      3402405      3377822      5487283       217389        67666        33203       121936          454
            5    80   50   1     17949200      1591440      1367075      2726511       177324        40895        23869        56202         4552
            6    96   84   2     25039488      7866030      7605202     19022100       150183        96444        40254       127493         3556
            7   112   72   2     25960928      4878810      4647016      6646579       277346        80409        39762        81691         3309
            8   128   63   2      4877696       345189       307082       887445        91917        17364         2600        18459          606
            9   160   51   2      3152000       470435       450735       712418        81161        19755         6687        13692          388
           10   192   63   3      7931712      1356246      1314935      2595825        81608        31104        18297        14202          792
           11   224   72   4       583072      1666870      1664267      4112733        94788        84418        18487        14044           37
           12   256   63   4      4441088       214118       196770       531195        50982        23215         2773         3298          332
           13   320   63   5       161600      1507044      1506539      3623214       112592        45548        16571        22779            9
           14   384   63   6     63520128      1115114       949697      1845693        61670        33546        15219         8725         3117
           15   448   63   7     56578368       922862       796571       908379        69788        43446        13528         7001         2006
           ...<cut>...
large:   size pages      nmalloc      ndalloc    nrequests      curruns
         4096     1          413          413          956            0
         8192     2          378          378          821            0
        12288     3           13           13           27            0
        16384     4            2            2            3            0
        20480     5            3            3            3            0
        ...<cut>...

 		 	   		   		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130426/81539833/attachment.html>

From abhishek at abhishek-singh.com  Sun Apr 28 21:03:36 2013
From: abhishek at abhishek-singh.com (Abhishek Singh)
Date: Mon, 29 Apr 2013 09:33:36 +0530
Subject: Excessive VM usage with jemalloc
Message-ID: <CAGCUJthh1KsK8g6TQ3Ub5NwDVFgB78_4dOQ=sb87o6njddGryw@mail.gmail.com>

Hi

We are trying to replace glibc malloc with jemalloc because we have several
concurrent allocations and in all our benchmarks jemalloc is consistently
better than glibc malloc and many others.

Our setups start typically with 96 GB of RAM and up. We have observed that
using jemalloc the virtual memory usage of our process rises up to around
75GB. While the resident memory stays low and it is not a problem as such,
when we try to fork a process from within here, it fails as the kernel
assumes there is not enough memory to copy the VM space. Perhaps a vfork
would be better but we can't use that for now.

So we have made some modifications to jemalloc so that all huge memory
allocations are forced to unmap their memory on freeing up. Non huge memory
allocations and freeing up remain the same. This seems to help us. I have
attached the patch here which is against jemalloc-3.3.1. Please review and
suggest if there is a better way to handle this.

-- 
Regards
Abhishek
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130429/ad69bdc4/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: jemalloc.patch
Type: application/octet-stream
Size: 5701 bytes
Desc: not available
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130429/ad69bdc4/attachment.obj>

From abhishek at abhishek-singh.com  Sun Apr 28 22:51:34 2013
From: abhishek at abhishek-singh.com (Abhishek Singh)
Date: Mon, 29 Apr 2013 11:21:34 +0530
Subject: Excessive VM usage with jemalloc
In-Reply-To: <CAGCUJthh1KsK8g6TQ3Ub5NwDVFgB78_4dOQ=sb87o6njddGryw@mail.gmail.com>
References: <CAGCUJthh1KsK8g6TQ3Ub5NwDVFgB78_4dOQ=sb87o6njddGryw@mail.gmail.com>
Message-ID: <CAGCUJtiVMRM0zcPNhL66P+XuJH=viHxPasPTVi65RUd2SD+Wcg@mail.gmail.com>

sending the patch inline as the mail system does not see to like
attachments.

diff -rupN jemalloc-3.3.1/include/jemalloc/internal/chunk.h
jemalloc-3.3.1_changed/include/jemalloc/internal/chunk.h
--- jemalloc-3.3.1/include/jemalloc/internal/chunk.h    2013-03-07
01:34:18.000000000 +0530
+++ jemalloc-3.3.1_changed/include/jemalloc/internal/chunk.h    2013-04-26
19:40:12.000000000 +0530
@@ -45,8 +45,8 @@ extern size_t         arena_maxclass; /* Max si

 void   *chunk_alloc(size_t size, size_t alignment, bool base, bool *zero,
     dss_prec_t dss_prec);
-void   chunk_unmap(void *chunk, size_t size);
-void   chunk_dealloc(void *chunk, size_t size, bool unmap);
+void   chunk_unmap(void *chunk, size_t size, bool force_unmap);
+void   chunk_dealloc(void *chunk, size_t size, bool unmap, bool
force_unmap);
 bool   chunk_boot(void);
 void   chunk_prefork(void);
 void   chunk_postfork_parent(void);
diff -rupN jemalloc-3.3.1/include/jemalloc/internal/chunk_mmap.h
jemalloc-3.3.1_changed/include/jemalloc/internal/chunk_mmap.h
--- jemalloc-3.3.1/include/jemalloc/internal/chunk_mmap.h       2013-03-07
01:34:18.000000000 +0530
+++ jemalloc-3.3.1_changed/include/jemalloc/internal/chunk_mmap.h
2013-04-26 19:34:36.000000000 +0530
@@ -12,7 +12,7 @@
 bool   pages_purge(void *addr, size_t length);

 void   *chunk_alloc_mmap(size_t size, size_t alignment, bool *zero);
-bool   chunk_dealloc_mmap(void *chunk, size_t size);
+bool   chunk_dealloc_mmap(void *chunk, size_t size, bool force_unmap);

 #endif /* JEMALLOC_H_EXTERNS */
 /******************************************************************************/
 /******************************************************************************/
Binary files jemalloc-3.3.1/lib/libjemalloc.so.2 and
jemalloc-3.3.1_changed/lib/libjemalloc.so.2 differ
diff -rupN jemalloc-3.3.1/src/arena.c jemalloc-3.3.1_changed/src/arena.c
--- jemalloc-3.3.1/src/arena.c  2013-03-07 01:34:18.000000000 +0530
+++ jemalloc-3.3.1_changed/src/arena.c  2013-04-26 19:40:33.000000000 +0530
@@ -617,7 +617,7 @@ arena_chunk_dealloc(arena_t *arena, aren

                arena->spare = chunk;
                malloc_mutex_unlock(&arena->lock);
-               chunk_dealloc((void *)spare, chunksize, true);
+               chunk_dealloc((void *)spare, chunksize, true, false);
                malloc_mutex_lock(&arena->lock);
                if (config_stats)
                        arena->stats.mapped -= chunksize;
diff -rupN jemalloc-3.3.1/src/chunk.c jemalloc-3.3.1_changed/src/chunk.c
--- jemalloc-3.3.1/src/chunk.c  2013-03-07 01:34:18.000000000 +0530
+++ jemalloc-3.3.1_changed/src/chunk.c  2013-04-26 19:39:33.000000000 +0530
@@ -104,7 +104,7 @@ chunk_recycle(extent_tree_t *chunks_szad
                        malloc_mutex_unlock(&chunks_mtx);
                        node = base_node_alloc();
                        if (node == NULL) {
-                               chunk_dealloc(ret, size, true);
+                               chunk_dealloc(ret, size, true, false);
                                return (NULL);
                        }
                        malloc_mutex_lock(&chunks_mtx);
@@ -181,7 +181,7 @@ label_return:
        if (ret != NULL) {
                if (config_ivsalloc && base == false) {
                        if (rtree_set(chunks_rtree, (uintptr_t)ret, ret)) {
-                               chunk_dealloc(ret, size, true);
+                               chunk_dealloc(ret, size, true, false);
                                return (NULL);
                        }
                }
@@ -288,7 +288,7 @@ chunk_record(extent_tree_t *chunks_szad,
 }

 void
-chunk_unmap(void *chunk, size_t size)
+chunk_unmap(void *chunk, size_t size, bool force_unmap)
 {
        assert(chunk != NULL);
        assert(CHUNK_ADDR2BASE(chunk) == chunk);
@@ -297,12 +297,12 @@ chunk_unmap(void *chunk, size_t size)

        if (config_dss && chunk_in_dss(chunk))
                chunk_record(&chunks_szad_dss, &chunks_ad_dss, chunk, size);
-       else if (chunk_dealloc_mmap(chunk, size))
+       else if (chunk_dealloc_mmap(chunk, size, force_unmap))
                chunk_record(&chunks_szad_mmap, &chunks_ad_mmap, chunk,
size);
 }

 void
-chunk_dealloc(void *chunk, size_t size, bool unmap)
+chunk_dealloc(void *chunk, size_t size, bool unmap, bool force_unmap)
 {

        assert(chunk != NULL);
@@ -320,7 +320,7 @@ chunk_dealloc(void *chunk, size_t size,
        }

        if (unmap)
-               chunk_unmap(chunk, size);
+               chunk_unmap(chunk, size, force_unmap);
 }

 bool
diff -rupN jemalloc-3.3.1/src/chunk_dss.c
jemalloc-3.3.1_changed/src/chunk_dss.c
--- jemalloc-3.3.1/src/chunk_dss.c      2013-03-07 01:34:18.000000000 +0530
+++ jemalloc-3.3.1_changed/src/chunk_dss.c      2013-04-26
19:38:11.000000000 +0530
@@ -123,7 +123,7 @@ chunk_alloc_dss(size_t size, size_t alig
                                dss_max = dss_next;
                                malloc_mutex_unlock(&dss_mtx);
                                if (cpad_size != 0)
-                                       chunk_unmap(cpad, cpad_size);
+                                       chunk_unmap(cpad, cpad_size, false);
                                if (*zero) {
                                        VALGRIND_MAKE_MEM_UNDEFINED(ret,
size);
                                        memset(ret, 0, size);
diff -rupN jemalloc-3.3.1/src/chunk_mmap.c
jemalloc-3.3.1_changed/src/chunk_mmap.c
--- jemalloc-3.3.1/src/chunk_mmap.c     2013-03-07 01:34:18.000000000 +0530
+++ jemalloc-3.3.1_changed/src/chunk_mmap.c     2013-04-26
19:48:15.000000000 +0530
@@ -200,11 +200,10 @@ chunk_alloc_mmap(size_t size, size_t ali
 }

 bool
-chunk_dealloc_mmap(void *chunk, size_t size)
+chunk_dealloc_mmap(void *chunk, size_t size, bool force_unmap)
 {
-
-       if (config_munmap)
+       if (config_munmap || force_unmap)
                pages_unmap(chunk, size);

-       return (config_munmap == false);
+       return (config_munmap == false) && (force_unmap == false);
 }
diff -rupN jemalloc-3.3.1/src/huge.c jemalloc-3.3.1_changed/src/huge.c
--- jemalloc-3.3.1/src/huge.c   2013-03-07 01:34:18.000000000 +0530
+++ jemalloc-3.3.1_changed/src/huge.c   2013-04-26 19:49:39.000000000 +0530
@@ -175,7 +175,7 @@ huge_ralloc(void *ptr, size_t oldsize, s
                        if (opt_abort)
                                abort();
                        memcpy(ret, ptr, copysize);
-                       chunk_dealloc_mmap(ptr, oldsize);
+                       chunk_dealloc_mmap(ptr, oldsize, true);
                }
        } else
 #endif
@@ -211,7 +211,7 @@ huge_dalloc(void *ptr, bool unmap)
        if (unmap && config_fill && config_dss && opt_junk)
                memset(node->addr, 0x5a, node->size);

-       chunk_dealloc(node->addr, node->size, unmap);
+       chunk_dealloc(node->addr, node->size, unmap, unmap);

        base_node_dealloc(node);
 }


On Mon, Apr 29, 2013 at 9:33 AM, Abhishek Singh <abhishek at abhishek-singh.com
> wrote:

> Hi
>
> We are trying to replace glibc malloc with jemalloc because we have
> several concurrent allocations and in all our benchmarks jemalloc is
> consistently better than glibc malloc and many others.
>
> Our setups start typically with 96 GB of RAM and up. We have observed that
> using jemalloc the virtual memory usage of our process rises up to around
> 75GB. While the resident memory stays low and it is not a problem as such,
> when we try to fork a process from within here, it fails as the kernel
> assumes there is not enough memory to copy the VM space. Perhaps a vfork
> would be better but we can't use that for now.
>
> So we have made some modifications to jemalloc so that all huge memory
> allocations are forced to unmap their memory on freeing up. Non huge memory
> allocations and freeing up remain the same. This seems to help us. I have
> attached the patch here which is against jemalloc-3.3.1. Please review and
> suggest if there is a better way to handle this.
>
> --
> Regards
> Abhishek
>



-- 
Regards
Abhishek Kumar Singh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130429/1be08daa/attachment.html>

From jasone at canonware.com  Tue Apr 30 22:52:24 2013
From: jasone at canonware.com (Jason Evans)
Date: Tue, 30 Apr 2013 22:52:24 -0700
Subject: jemalloc performance for very small allocations
In-Reply-To: <BAY154-W185E683D345520A347B5EDEFB70@phx.gbl>
References: <BAY154-W8749FBE688DE4A8A678A3EFB70@phx.gbl>
	<BAY154-W185E683D345520A347B5EDEFB70@phx.gbl>
Message-ID: <F7B11C0C-EFC3-4104-BA1D-0677BD43369D@canonware.com>

On Apr 26, 2013, at 9:39 AM, Rajat Garg <garg_rajat at hotmail.com> wrote:
>  I am using jemalloc 3.1.0, compiled with gcc 4.1.2 on CentOS release 5.6 for a compute intensive multithreaded application where bulk of allocations falls in 8-byte, 16-byte (stats below). We are seeing very high runtime in tcache_alloc_small_hard(), arena_tcache_fill_small() and arena_bin_malloc_hard().  The 8 and 16 bytes allocations happen in very large number. We probably want the allocations to come form tcache_alloc_easy() to not hit the locks and take less runtime.  The allocations are mostly temporary in nature -- especially 8-byte ones -- so some number of 8-byte allocations are done, then they are freed and then alloc/free process repeated. 
> 
> The runs use 8 threads (so 48 arenas and all other jemalloc settings are default) and are on an Intel Xeon server with 12-cores (no hyperthreading and no other user so no contention with other user; the peak memory in application is ~6.5GB and server has 128GB memory so no memory shortage/swapping etc.); the jemalloc output at the end of run is given below. As we can see, pretty much 8 and 16 byte bins are overloaded.

Only 8 of the 48 arenas are being used in your test case.  That said, the extra 40 are lazily initialized, so there's no need to change settings.

> A run with TCACHE_NSLOTS_SMALL_MAX=10000, LG_TCACHE_MAXCLASS_DEFAULT=10, changing small bins to hold only upto 224 bytes (NBINS=15),  and hardcoding 
> tcache_bin_info[i].ncached_max = TCACHE_NSLOTS_SMALL_MAX in tcache.c file improves runtime by about 16% (for overall application runtime) but increases peak memory from 6.5GB to 6.7GB. We want the runtime to at least improve by another 10% without preferably any increase in peak memory (so even 6.5GB to 6.7GB is not desirable). 
> Any suggestions on what changes to jemalloc settings to try?

I don't think the TCACHE_NSLOTS_SMALL_MAX setting is having as much effect for your test case as you might expect.  Even with the setting of 10000, tcache is limited to 1002 and 504 regions for 8- and 16-byte regions, respectively.  If you want to further increase the tcache region count, you will need to either increase the logical page size, or modify bin_info_run_size_calc() to use a larger initial min_run_size.

Jason
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130430/fbe3aeba/attachment.html>

