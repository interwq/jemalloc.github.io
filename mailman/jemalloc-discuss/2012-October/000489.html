<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> Memory usage regression
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20Memory%20usage%20regression&In-Reply-To=%3C20121030181153.GB921%40glandium.org%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000488.html">
   <LINK REL="Next"  HREF="000490.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>Memory usage regression</H1>
    <B>Mike Hommey</B> 
    <A HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20Memory%20usage%20regression&In-Reply-To=%3C20121030181153.GB921%40glandium.org%3E"
       TITLE="Memory usage regression">mh+jemalloc at glandium.org
       </A><BR>
    <I>Tue Oct 30 11:11:53 PDT 2012</I>
    <P><UL>
        <LI>Previous message: <A HREF="000488.html">Memory usage regression
</A></li>
        <LI>Next message: <A HREF="000490.html">Memory usage regression
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#489">[ date ]</a>
              <a href="thread.html#489">[ thread ]</a>
              <a href="subject.html#489">[ subject ]</a>
              <a href="author.html#489">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On Tue, Oct 30, 2012 at 07:01:21PM +0100, Mike Hommey wrote:
&gt;<i> On Tue, Oct 30, 2012 at 05:03:38PM +0100, Mike Hommey wrote:
</I>&gt;<i> &gt; So, what seems to be happening is that because of that fragmentation, when
</I>&gt;<i> &gt; requesting big allocations, jemalloc has to allocate and use new chunks.
</I>&gt;<i> &gt; When these big allocations are freed, the new chunk tends to be used
</I>&gt;<i> &gt; more often than the other free chunks, adding to the fragmentation, thus
</I>&gt;<i> &gt; requiring more new chunks for big allocations.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; The following dumb patch essentially plugs the leak for the Firefox usecase:
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; diff --git a/src/arena.c b/src/arena.c
</I>&gt;<i> &gt; index 1e6964a..38079d7 100644
</I>&gt;<i> &gt; --- a/src/arena.c
</I>&gt;<i> &gt; +++ b/src/arena.c
</I>&gt;<i> &gt; @@ -471,7 +471,7 @@ arena_run_alloc_helper(arena_t *arena, size_t size, bool large, size_t binind,
</I>&gt;<i> &gt;         arena_chunk_map_t *mapelm, key;
</I>&gt;<i> &gt;  
</I>&gt;<i> &gt;         key.bits = size | CHUNK_MAP_KEY;
</I>&gt;<i> &gt; -       mapelm = arena_avail_tree_nsearch(&amp;arena-&gt;runs_avail_dirty, &amp;key);
</I>&gt;<i> &gt; +       mapelm = arena_avail_tree_nsearch(&amp;arena-&gt;runs_avail_clean, &amp;key);
</I>&gt;<i> &gt;         if (mapelm != NULL) {
</I>&gt;<i> &gt;                 arena_chunk_t *run_chunk = CHUNK_ADDR2BASE(mapelm);
</I>&gt;<i> &gt;                 size_t pageind = (((uintptr_t)mapelm -
</I>&gt;<i> &gt; @@ -483,7 +483,7 @@ arena_run_alloc_helper(arena_t *arena, size_t size, bool large, size_t binind,
</I>&gt;<i> &gt;                 arena_run_split(arena, run, size, large, binind, zero);
</I>&gt;<i> &gt;                 return (run);
</I>&gt;<i> &gt;         }
</I>&gt;<i> &gt; -       mapelm = arena_avail_tree_nsearch(&amp;arena-&gt;runs_avail_clean, &amp;key);
</I>&gt;<i> &gt; +       mapelm = arena_avail_tree_nsearch(&amp;arena-&gt;runs_avail_dirty, &amp;key);
</I>&gt;<i> &gt;         if (mapelm != NULL) {
</I>&gt;<i> &gt;                 arena_chunk_t *run_chunk = CHUNK_ADDR2BASE(mapelm);
</I>&gt;<i> &gt;                 size_t pageind = (((uintptr_t)mapelm -
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; My test program changed in the meanwhile, so i can't do accurate
</I>&gt;<i> &gt; comparisons with mozjemalloc without re-running more tests. I'll post
</I>&gt;<i> &gt; again when I have more data.
</I>
Oops. There was supposed to be an url around here:
<A HREF="http://i.imgur.com/A0len.png">http://i.imgur.com/A0len.png</A>

&gt;<i> Here's some comparison between jemalloc3 (tcache=false, narenas=1,
</I>&gt;<i> lg_chunk=20) with the patch above and mozjemalloc.
</I>&gt;<i> The graph shows (mozjemalloc - jemalloc3) / jemalloc3 for each value.
</I>&gt;<i> 
</I>&gt;<i> mozjemalloc has a lower rss after closing tabs, but jemalloc3 has a
</I>&gt;<i> lower rss when all tabs are opened. VmData difference is in an
</I>&gt;<i> acceptable range.
</I>&gt;<i> &quot;Usable&quot; is the total of malloc_usable_size() for all allocations, which
</I>&gt;<i> means jemalloc allocates more &quot;unrequested&quot; memory than mozjemalloc.
</I>&gt;<i> That could certainly contribute to some additional fragmentation and
</I>&gt;<i> to the RSS difference, by extension.
</I>&gt;<i> 
</I>&gt;<i> Mike
</I>
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000488.html">Memory usage regression
</A></li>
	<LI>Next message: <A HREF="000490.html">Memory usage regression
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#489">[ date ]</a>
              <a href="thread.html#489">[ thread ]</a>
              <a href="subject.html#489">[ subject ]</a>
              <a href="author.html#489">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">More information about the jemalloc-discuss
mailing list</a><br>
</body></html>
