<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> High amount of private clean data in smaps
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20High%20amount%20of%20private%20clean%20data%20in%20smaps&In-Reply-To=%3COF65339387.A3A4F4B3-ON87257B95.004830B3-85257B95.0048B110%40us.ibm.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000605.html">
   <LINK REL="Next"  HREF="000610.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>High amount of private clean data in smaps</H1>
    <B>Thomas R Gissel</B> 
    <A HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20High%20amount%20of%20private%20clean%20data%20in%20smaps&In-Reply-To=%3COF65339387.A3A4F4B3-ON87257B95.004830B3-85257B95.0048B110%40us.ibm.com%3E"
       TITLE="High amount of private clean data in smaps">gissel at us.ibm.com
       </A><BR>
    <I>Tue Jun 25 06:13:21 PDT 2013</I>
    <P><UL>
        <LI>Previous message: <A HREF="000605.html">High amount of private clean data in smaps
</A></li>
        <LI>Next message: <A HREF="000610.html">High amount of private clean data in smaps
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#609">[ date ]</a>
              <a href="thread.html#609">[ thread ]</a>
              <a href="subject.html#609">[ subject ]</a>
              <a href="author.html#609">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>
With help from our local Linux kernel experts we've tracked down the
inexplicable Private_Clean emergence in our processes' smaps file to a
kernel bug,
<A HREF="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/fs/proc/task_mmu.c?id=1c2499ae87f828eabddf6483b0dfc11da1100c07">https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/fs/proc/task_mmu.c?id=1c2499ae87f828eabddf6483b0dfc11da1100c07</A>
, which, according to GIT, was first&#160;committed&#160;in&#160;v2.6.36-rc6~63. &#160;When we
manually applied the aforementioned patched to our&#160;kernel&#160;there were no
memory segments in smaps showing large Private_Clean regions during our
test. &#160;Unfortunately&#160;the fix seems to have been merely an accounting
change. &#160;Everything&#160;previously reported as Private_Clean is now correctly
showing up as Private_Dirty, so we are still digging to find out why our
RSS, specifically Private_Dirty, continues to grow while jemalloc's active
reports much lower numbers.

Thanks,

Tom


|<i>------------&gt;
</I>|<i> From:      |
</I>|<i>------------&gt;
</I>  &gt;-------------------------------------------------------------------------------------------------------------------------------------------------|
  |Jason Evans &lt;<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">jasone at canonware.com</A>&gt;                                                                                                               |
  &gt;-------------------------------------------------------------------------------------------------------------------------------------------------|
|<i>------------&gt;
</I>|<i> To:        |
</I>|<i>------------&gt;
</I>  &gt;-------------------------------------------------------------------------------------------------------------------------------------------------|
  |Thomas R Gissel/Rochester/<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">IBM at IBMUS</A>,                                                                                                             |
  &gt;-------------------------------------------------------------------------------------------------------------------------------------------------|
|<i>------------&gt;
</I>|<i> Cc:        |
</I>|<i>------------&gt;
</I>  &gt;-------------------------------------------------------------------------------------------------------------------------------------------------|
  |<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">jemalloc-discuss at canonware.com</A>                                                                                                                   |
  &gt;-------------------------------------------------------------------------------------------------------------------------------------------------|
|<i>------------&gt;
</I>|<i> Date:      |
</I>|<i>------------&gt;
</I>  &gt;-------------------------------------------------------------------------------------------------------------------------------------------------|
  |06/06/2013 01:34 AM                                                                                                                              |
  &gt;-------------------------------------------------------------------------------------------------------------------------------------------------|
|<i>------------&gt;
</I>|<i> Subject:   |
</I>|<i>------------&gt;
</I>  &gt;-------------------------------------------------------------------------------------------------------------------------------------------------|
  |Re: High amount of private clean data in smaps                                                                                                   |
  &gt;-------------------------------------------------------------------------------------------------------------------------------------------------|





On Jun 5, 2013, at 9:17 PM, Thomas R Gissel &lt;<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">gissel at us.ibm.com</A>&gt; wrote:


      I too have been trying to reproduce the existence of Private_Clean
      memory segments in smaps via a simple test case with jemalloc and was
      unable to on my laptop, a 2 core machine running a 3.8.0-23 kernel .
      I then moved my test to our production box: 96GB memory, 24 hardware
      threads and 2.6 kernel (detailed information below), and within a few
      minutes of execution, with a few minor adjustments, I was able
      duplicate the results, smaps showing the jemalloc segment with
      Private_Clean memory usage, of our larger test. Note that I'm using
      the same jemalloc library whose information Kurtis posted earlier (96
      arenas etc...).


Interesting!  I don't see anything unusual about the test program, so I'm
guessing this is kernel-specific.  I'll run it on some 8- and 16-core
machines tomorrow with a couple of kernel versions and see what happens.

Thanks,
Jason
-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130625/3215e254/attachment.html">http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130625/3215e254/attachment.html</A>&gt;
-------------- next part --------------
A non-text attachment was scrubbed...
Name: graycol.gif
Type: image/gif
Size: 105 bytes
Desc: not available
URL: &lt;<A HREF="http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130625/3215e254/attachment.gif">http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130625/3215e254/attachment.gif</A>&gt;
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ecblank.gif
Type: image/gif
Size: 45 bytes
Desc: not available
URL: &lt;<A HREF="http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130625/3215e254/attachment-0001.gif">http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130625/3215e254/attachment-0001.gif</A>&gt;
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000605.html">High amount of private clean data in smaps
</A></li>
	<LI>Next message: <A HREF="000610.html">High amount of private clean data in smaps
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#609">[ date ]</a>
              <a href="thread.html#609">[ thread ]</a>
              <a href="subject.html#609">[ subject ]</a>
              <a href="author.html#609">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">More information about the jemalloc-discuss
mailing list</a><br>
</body></html>
