<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> High amount of private clean data in smaps
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20High%20amount%20of%20private%20clean%20data%20in%20smaps&In-Reply-To=%3COF519EDCD4.68687AF6-ON87257B82.001634F4-85257B82.0017A177%40us.ibm.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000602.html">
   <LINK REL="Next"  HREF="000605.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>High amount of private clean data in smaps</H1>
    <B>Thomas R Gissel</B> 
    <A HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20High%20amount%20of%20private%20clean%20data%20in%20smaps&In-Reply-To=%3COF519EDCD4.68687AF6-ON87257B82.001634F4-85257B82.0017A177%40us.ibm.com%3E"
       TITLE="High amount of private clean data in smaps">gissel at us.ibm.com
       </A><BR>
    <I>Wed Jun  5 21:17:46 PDT 2013</I>
    <P><UL>
        <LI>Previous message: <A HREF="000602.html">High amount of private clean data in smaps
</A></li>
        <LI>Next message: <A HREF="000605.html">High amount of private clean data in smaps
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#604">[ date ]</a>
              <a href="thread.html#604">[ thread ]</a>
              <a href="subject.html#604">[ subject ]</a>
              <a href="author.html#604">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>

I too have been trying to reproduce the existence of Private_Clean
memory&#160;segments in smaps via a simple test case with jemalloc and was
unable to on my laptop, a 2 core machine running a 3.8.0-23 kernel . &#160;I
then moved my test to our production box: 96GB memory, 24 hardware threads
and 2.6 kernel (detailed information below), and within a few minutes of
execution, with a few minor adjustments, I was able duplicate the results,
smaps showing the jemalloc segment with Private_Clean memory usage, of our
larger test. Note that I'm using the same jemalloc library whose
information Kurtis posted earlier (96 arenas etc...).

Thank you,
Tom

---------------
DETAILS


Smaps memory segment
&#160;7f1f75c00000-7f35bce00000 rw-p 00000000 00:00 0
Size: 93440000 kB
Rss: 63177024 kB
Pss: 63177024 kB
Shared_Clean: 0 kB
Shared_Dirty: 0 kB
Private_Clean: 614992 kB
Private_Dirty: 62562032 kB
Referenced: 62562212 kB
Swap: 0 kB
KernelPageSize: 4 kB
MMUPageSize: 4 kB


Production Box Info:
# uname -a
Linux 9.42.94.12 2.6.32-131.21.1.89.br5_0.x86_64 #1 SMP Tue Mar 19 15:05:57
CDT 2013 x86_64 GNU/Linux

Tail of CPU Info
processor&#160;&#160;&#160;&#160;&#160;&#160; : 23
vendor_id&#160;&#160;&#160;&#160;&#160;&#160; : GenuineIntel
cpu family&#160;&#160;&#160;&#160;&#160; : 6
model&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; : 44
model name&#160;&#160;&#160;&#160;&#160; : Intel(R) Xeon(R) CPU&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; X5670&#160; @ 2.93GHz
stepping&#160;&#160;&#160;&#160;&#160;&#160;&#160; : 2
cpu MHz&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; : 2933.304
cache size&#160;&#160;&#160;&#160;&#160; : 12288 KB
physical id&#160;&#160;&#160;&#160; : 1
siblings&#160;&#160;&#160;&#160;&#160;&#160;&#160; : 12
core id&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; : 10
cpu cores&#160;&#160;&#160;&#160;&#160;&#160; : 6
apicid&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; : 53
initial apicid&#160; : 53
fpu&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; : yes
fpu_exception&#160;&#160; : yes
cpuid level&#160;&#160;&#160;&#160; : 11
wp&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; : yes
flags&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; : fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov
pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx
pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology
nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2
ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt aes lahf_lm ida arat
tpr_shadow vnmi flexpriority ept vpid
bogomips&#160;&#160;&#160;&#160;&#160;&#160;&#160; : 5867.23
clflush size&#160;&#160;&#160; : 64
cache_alignment : 64
address sizes&#160;&#160; : 40 bits physical, 48 bits virtual
power management:


(See attached file: stressTest.c)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://jemalloc.net/mailman/jemalloc-discuss/attachments/20130605/d2faaf22/attachment.html">http://jemalloc.net/mailman/jemalloc-discuss/attachments/20130605/d2faaf22/attachment.html</A>&gt;
-------------- next part --------------
A non-text attachment was scrubbed...
Name: stressTest.c
Type: application/octet-stream
Size: 5813 bytes
Desc: not available
URL: &lt;<A HREF="http://jemalloc.net/mailman/jemalloc-discuss/attachments/20130605/d2faaf22/attachment.obj">http://jemalloc.net/mailman/jemalloc-discuss/attachments/20130605/d2faaf22/attachment.obj</A>&gt;
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000602.html">High amount of private clean data in smaps
</A></li>
	<LI>Next message: <A HREF="000605.html">High amount of private clean data in smaps
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#604">[ date ]</a>
              <a href="thread.html#604">[ thread ]</a>
              <a href="subject.html#604">[ subject ]</a>
              <a href="author.html#604">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">More information about the jemalloc-discuss
mailing list</a><br>
</body></html>
