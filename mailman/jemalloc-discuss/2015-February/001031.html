<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> jemalloc 3 performance vs. mozjemalloc
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20jemalloc%203%20performance%20vs.%20mozjemalloc&In-Reply-To=%3C20150210043210.GA25825%40glandium.org%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001022.html">
   <LINK REL="Next"  HREF="001032.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>jemalloc 3 performance vs. mozjemalloc</H1>
    <B>Mike Hommey</B> 
    <A HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20jemalloc%203%20performance%20vs.%20mozjemalloc&In-Reply-To=%3C20150210043210.GA25825%40glandium.org%3E"
       TITLE="jemalloc 3 performance vs. mozjemalloc">mh at glandium.org
       </A><BR>
    <I>Mon Feb  9 20:32:10 PST 2015</I>
    <P><UL>
        <LI>Previous message: <A HREF="001022.html">jemalloc 3 performance vs. mozjemalloc
</A></li>
        <LI>Next message: <A HREF="001032.html">jemalloc 3 performance vs. mozjemalloc
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1031">[ date ]</a>
              <a href="thread.html#1031">[ thread ]</a>
              <a href="subject.html#1031">[ subject ]</a>
              <a href="author.html#1031">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On Wed, Feb 04, 2015 at 10:15:37AM -0800, Jason Evans wrote:
&gt;<i> On Feb 3, 2015, at 4:40 PM, Mike Hommey &lt;<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">mh at glandium.org</A>&gt; wrote:
</I>&gt;<i> &gt; On Tue, Feb 03, 2015 at 04:19:00PM -0800, Jason Evans wrote:
</I>&gt;<i> &gt;&gt; On Feb 3, 2015, at 2:51 PM, Mike Hommey &lt;<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">mh at glandium.org</A>&gt; wrote:
</I>&gt;<i> &gt;&gt;&gt; I've been tracking a startup time regression in Firefox for
</I>&gt;<i> &gt;&gt;&gt; Android when we tried to switch from mozjemalloc (memory
</I>&gt;<i> &gt;&gt;&gt; refresher: it's derived from jemalloc 0.9) to mostly current
</I>&gt;<i> &gt;&gt;&gt; jemalloc dev.
</I>&gt;<i> &gt;&gt;&gt; 
</I>&gt;<i> &gt;&gt;&gt; It turned out to be <A HREF="https://github.com/jemalloc/jemalloc/pull/192">https://github.com/jemalloc/jemalloc/pull/192</A>
</I>&gt;<i> &gt;&gt; 
</I>&gt;<i> &gt;&gt; I intentionally removed the functionality #192 adds back (in
</I>&gt;<i> &gt;&gt; e3d13060c8a04f08764b16b003169eb205fa09eb), but apparently forgot to
</I>&gt;<i> &gt;&gt; update the documentation.  Do you have an understanding of why it's
</I>&gt;<i> &gt;&gt; hurting performance so much?
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; My understanding is that the huge increase in page faults is making
</I>&gt;<i> &gt; the difference. On Firefox startup we go from 50k page faults to 35k
</I>&gt;<i> &gt; with that patch. I can surely double check whether it's really the
</I>&gt;<i> &gt; page faults, or if it's actually the madvising itself that causes
</I>&gt;<i> &gt; the regression. Or both.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt;&gt; Is there any chance you can make your test case available so I can
</I>&gt;<i> &gt;&gt; dig in further?
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; <A HREF="https://gist.githubusercontent.com/glandium/a42d0265e324688cafc4/raw/gistfile1.c">https://gist.githubusercontent.com/glandium/a42d0265e324688cafc4/raw/gistfile1.c</A>
</I>&gt;<i> 
</I>&gt;<i> I added some logging and determined that ~90% of the dirty page
</I>&gt;<i> purging is happening in the first 2% of the allocation trace.  This
</I>&gt;<i> appears to be almost entirely due to repeated 32 KiB
</I>&gt;<i> allocation/deallocation.
</I>
So, interestingly, this appears to be a bug that was intended to have
been fixed, but wasn't (the repeated allocation/deallocation of 32kiB
buffers). Fixing that, however, still leaves us with a big difference in
the number of page faults (but lower than before), but now the dirty
page purging threshold patch seems to have less impact than it did...

I haven't analyzed these builds further yet, so I can't really tell much
more at the moment.

&gt;<i> I still have vague plans to add time-based hysteresis mechanisms so
</I>&gt;<i> that #192 isn't necessary, but until then, #192 it is.
</I>
Sadly, #192 also makes the RSS footprint bigger when using more than one
arena. With 4 cores, so 16 arenas, and default 4MB chunks, that's 64MB
of memory that won't be purged. It's not a problem for us because we use
1MB chunks and 1 arena, but I can see this being a problem with the
default settings.

FWIW, I also tried to remove all the bin mutexes, and make them all use
the arena mutex, and, counter-intuitively, it made things faster. Not by
a very significant margin, though, but it's interesting to note that the
synchronization overheads of n locks can make things slower than 1
lock with more contention.

IOW, I'm still searching for what's wrong :(

Mike
</PRE>


<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001022.html">jemalloc 3 performance vs. mozjemalloc
</A></li>
	<LI>Next message: <A HREF="001032.html">jemalloc 3 performance vs. mozjemalloc
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1031">[ date ]</a>
              <a href="thread.html#1031">[ thread ]</a>
              <a href="subject.html#1031">[ subject ]</a>
              <a href="author.html#1031">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">More information about the jemalloc-discuss
mailing list</a><br>
</body></html>
