<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> Excessive VM usage with jemalloc
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20Excessive%20VM%20usage%20with%20jemalloc&In-Reply-To=%3CCAGCUJtgAtyP77DGn0UkvscLu%3DWneXQ6KVXpPXRCYoASxfBta1g%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000581.html">
   <LINK REL="Next"  HREF="000583.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>Excessive VM usage with jemalloc</H1>
    <B>Abhishek Singh</B> 
    <A HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20Excessive%20VM%20usage%20with%20jemalloc&In-Reply-To=%3CCAGCUJtgAtyP77DGn0UkvscLu%3DWneXQ6KVXpPXRCYoASxfBta1g%40mail.gmail.com%3E"
       TITLE="Excessive VM usage with jemalloc">abhishek at abhishek-singh.com
       </A><BR>
    <I>Mon May  6 06:46:41 PDT 2013</I>
    <P><UL>
        <LI>Previous message: <A HREF="000581.html">Excessive VM usage with jemalloc
</A></li>
        <LI>Next message: <A HREF="000583.html">Excessive VM usage with jemalloc
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#582">[ date ]</a>
              <a href="thread.html#582">[ thread ]</a>
              <a href="subject.html#582">[ subject ]</a>
              <a href="author.html#582">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Hi

Yes, results with --enable-munmap are similar but some preliminary
benchmarks for the kind of allocations we do, seem to indicate degradation
in performance as compared to unmap disabled. While you are right and
ideally it would be great to unmap all the memory as and when it is not
needed, if there is a performance tradeoff then it may not be feasible. Our
aim as of now is to ensure that we cause minimal memory fragmentation
(thereby reducing our virtual memory footprint) and still get as good a
performance as we can. Selectively unmapping only huge allocations seemed
to be a compromise between the two extremes.

--
Abhishek


On Sat, May 4, 2013 at 10:14 PM, Jason Evans &lt;<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">jasone at canonware.com</A>&gt; wrote:

&gt;<i> On Apr 28, 2013, at 9:03 PM, Abhishek Singh &lt;<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">abhishek at abhishek-singh.com</A>&gt;
</I>&gt;<i> wrote:
</I>&gt;<i> &gt; We are trying to replace glibc malloc with jemalloc because we have
</I>&gt;<i> several concurrent allocations and in all our benchmarks jemalloc is
</I>&gt;<i> consistently better than glibc malloc and many others.
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Our setups start typically with 96 GB of RAM and up. We have observed
</I>&gt;<i> that using jemalloc the virtual memory usage of our process rises up to
</I>&gt;<i> around 75GB. While the resident memory stays low and it is not a problem as
</I>&gt;<i> such, when we try to fork a process from within here, it fails as the
</I>&gt;<i> kernel assumes there is not enough memory to copy the VM space. Perhaps a
</I>&gt;<i> vfork would be better but we can't use that for now.
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; So we have made some modifications to jemalloc so that all huge memory
</I>&gt;<i> allocations are forced to unmap their memory on freeing up. Non huge memory
</I>&gt;<i> allocations and freeing up remain the same. This seems to help us. I have
</I>&gt;<i> attached the patch here which is against jemalloc-3.3.1. Please review and
</I>&gt;<i> suggest if there is a better way to handle this.
</I>&gt;<i>
</I>&gt;<i> Does --enable-munmap give you similar results?  Ideally, munmap() would
</I>&gt;<i> always be enabled, but Linux has some unfortunate VM map fragmentation
</I>&gt;<i> issues (it doesn't consistently reuse holes left by munmap()).  I don't
</I>&gt;<i> think there's much benefit to using munmap() only selectively, because
</I>&gt;<i> avoiding the VM map fragmentation issue is an all-or-nothing proposition.
</I>&gt;<i>
</I>&gt;<i> At times I've considered adding pre-fork() code that unmaps everything
</I>&gt;<i> possible, and purges unused dirty pages.  The problem with this is that
</I>&gt;<i> it's expensive, and it doesn't pay off if the process does exec() right
</I>&gt;<i> after fork() -- the common case.
</I>&gt;<i>
</I>&gt;<i> Jason
</I>-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130506/292cb3a3/attachment.html">http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20130506/292cb3a3/attachment.html</A>&gt;
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000581.html">Excessive VM usage with jemalloc
</A></li>
	<LI>Next message: <A HREF="000583.html">Excessive VM usage with jemalloc
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#582">[ date ]</a>
              <a href="thread.html#582">[ thread ]</a>
              <a href="subject.html#582">[ subject ]</a>
              <a href="author.html#582">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">More information about the jemalloc-discuss
mailing list</a><br>
</body></html>
