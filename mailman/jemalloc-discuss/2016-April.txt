From cpatti at tintri.com  Mon Apr 18 23:12:25 2016
From: cpatti at tintri.com (Chaitanya Patti)
Date: Tue, 19 Apr 2016 06:12:25 +0000
Subject: One run extending into another in jemalloc-3.5.1 ?
Message-ID: <SN1PR0701MB1967C76CDB9F6FF33BB80265B26C0@SN1PR0701MB1967.namprd07.prod.outlook.com>

Hi,


I am debugging a memory de-allocation issue. We are using jemalloc version 3.5.1. It looks like a run with reg_size 224 and total size of 4 pages has "extended" into an adjacent run, and corrupted the adjacent run. Has such an issue been seen before ?


Thanks,

Chaitanya.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20160419/1d5d817d/attachment.html>

From jasone at canonware.com  Tue Apr 19 10:01:32 2016
From: jasone at canonware.com (Jason Evans)
Date: Tue, 19 Apr 2016 10:01:32 -0700
Subject: One run extending into another in jemalloc-3.5.1 ?
In-Reply-To: <SN1PR0701MB1967C76CDB9F6FF33BB80265B26C0@SN1PR0701MB1967.namprd07.prod.outlook.com>
References: <SN1PR0701MB1967C76CDB9F6FF33BB80265B26C0@SN1PR0701MB1967.namprd07.prod.outlook.com>
Message-ID: <2CFA433E-8DAF-4884-B7B3-3B6CDF5F14DE@canonware.com>

On Apr 18, 2016, at 11:12 PM, Chaitanya Patti <cpatti at tintri.com> wrote:
> I am debugging a memory de-allocation issue. We are using jemalloc version 3.5.1. It looks like a run with reg_size 224 and total size of 4 pages has "extended" into an adjacent run, and corrupted the adjacent run. Has such an issue been seen before ? 

That usually means that a double free corrupted metadata for the adjacent run.  If you have a repeatable test case, try running with a debug build of jemalloc, and disable tcache, so that assertions immediately detect double frees.

Jason

From cpatti at tintri.com  Tue Apr 19 10:48:34 2016
From: cpatti at tintri.com (Chaitanya Patti)
Date: Tue, 19 Apr 2016 17:48:34 +0000
Subject: One run extending into another in jemalloc-3.5.1 ?
In-Reply-To: <2CFA433E-8DAF-4884-B7B3-3B6CDF5F14DE@canonware.com>
References: <SN1PR0701MB1967C76CDB9F6FF33BB80265B26C0@SN1PR0701MB1967.namprd07.prod.outlook.com>,
	<2CFA433E-8DAF-4884-B7B3-3B6CDF5F14DE@canonware.com>
Message-ID: <SN1PR0701MB1967E96B601383882D9B5D11B26C0@SN1PR0701MB1967.namprd07.prod.outlook.com>

I don't understand how a double-free can cause such a corruption. Can you please explain ?

________________________________________
From: Jason Evans <jasone at canonware.com>
Sent: Tuesday, April 19, 2016 10:01 AM
To: Chaitanya Patti
Cc: jemalloc-discuss at canonware.com
Subject: Re: One run extending into another in jemalloc-3.5.1 ?

On Apr 18, 2016, at 11:12 PM, Chaitanya Patti <cpatti at tintri.com> wrote:
> I am debugging a memory de-allocation issue. We are using jemalloc version 3.5.1. It looks like a run with reg_size 224 and total size of 4 pages has "extended" into an adjacent run, and corrupted the adjacent run. Has such an issue been seen before ?

That usually means that a double free corrupted metadata for the adjacent run.  If you have a repeatable test case, try running with a debug build of jemalloc, and disable tcache, so that assertions immediately detect double frees.

Jason

From cpatti at tintri.com  Tue Apr 19 11:26:35 2016
From: cpatti at tintri.com (Chaitanya Patti)
Date: Tue, 19 Apr 2016 18:26:35 +0000
Subject: One run extending into another in jemalloc-3.5.1 ?
In-Reply-To: <2CFA433E-8DAF-4884-B7B3-3B6CDF5F14DE@canonware.com>
References: <SN1PR0701MB1967C76CDB9F6FF33BB80265B26C0@SN1PR0701MB1967.namprd07.prod.outlook.com>,
	<2CFA433E-8DAF-4884-B7B3-3B6CDF5F14DE@canonware.com>
Message-ID: <32B34AE2-93CB-466E-A6C6-81BEE5C8BD31@tintri.com>

Let us say "run1" has extended into "run2". Are you saying that the metadata of run1 has been corrupted by a double free ?

--Chaitanya 

> On Apr 19, 2016, at 10:01, Jason Evans <jasone at canonware.com> wrote:
> 
>> On Apr 18, 2016, at 11:12 PM, Chaitanya Patti <cpatti at tintri.com> wrote:
>> I am debugging a memory de-allocation issue. We are using jemalloc version 3.5.1. It looks like a run with reg_size 224 and total size of 4 pages has "extended" into an adjacent run, and corrupted the adjacent run. Has such an issue been seen before ?
> 
> That usually means that a double free corrupted metadata for the adjacent run.  If you have a repeatable test case, try running with a debug build of jemalloc, and disable tcache, so that assertions immediately detect double frees.
> 
> Jason

From cpatti at tintri.com  Thu Apr 21 09:35:54 2016
From: cpatti at tintri.com (Chaitanya Patti)
Date: Thu, 21 Apr 2016 16:35:54 +0000
Subject: One run extending into another in jemalloc-3.5.1 ?
In-Reply-To: <32B34AE2-93CB-466E-A6C6-81BEE5C8BD31@tintri.com>
References: <SN1PR0701MB1967C76CDB9F6FF33BB80265B26C0@SN1PR0701MB1967.namprd07.prod.outlook.com>,
	<2CFA433E-8DAF-4884-B7B3-3B6CDF5F14DE@canonware.com>,
	<32B34AE2-93CB-466E-A6C6-81BEE5C8BD31@tintri.com>
Message-ID: <SN1PR0701MB1967695A7D37DD3B290753AEB26E0@SN1PR0701MB1967.namprd07.prod.outlook.com>

I thought I would provide more detail about what we are seeing:

(gdb) p arena_bin_info[11]
$16 = {
  reg_size = 224, 
  redzone_size = 0, 
  reg_interval = 224, 
  run_size = 16384, 
  nregs = 72, 
  bitmap_offset = 16, 
  bitmap_info = {
    nbits = 72, 
    nlevels = 2, 
    levels = {{
        group_offset = 0
      }, {
        group_offset = 2
      }, {
        group_offset = 3
      }, {
        group_offset = 0
      }, {
        group_offset = 0
      }}
  }, 
  ctx0_offset = 0, 
  reg0_offset = 256
}

The above arena_bin_info says that the run size of this size class is 16384 (
i.e. 4 pages ). 

Also, there is a header of 256 (reg0_offset) on the first page.
And there are contiguous alloctions of 224 bytes after that.

So, the offset, within the page, of every allocation on the first page is of
the form ( 256 + 224 * n).
The offset, within the page, of every allocation on the second page will be of
the form ( 192 + 224 * n ).
The offset, within the page, of every allocation on the third page will be of
the form ( 128 + 224 * n ).
The offset, within the page, of every allocation on the fourth page will be of
the form ( 64 + 224 * n ).

We see that 200 byte objects ( these will fall in the reg_size = 224 bin ) have been allocated at the following addresses:

0x7f1edc7a1380
0x7f1edc7a1460
0x7f1edc7a1540
0x7f1edc7a17e0
0x7f1edc7a18c0
0x7f1edc7a1a80
0x7f1edc7a1b60
0x7f1edc7a1c40

The above addresses, do not fit any of
these forms we mentioned above ( x + 224 * n ). 
So, they cannot be pointing to addresses in this size class.

But if we consider that a fifth page were allowed in a run of this size class,
then, the offset, within the page, of every allocation in that fifth page will
be of the form ( 224 * n ), and that fits the addresses we have!

If we were to consider the page starting at 0x7f1edc7a1000 as the fifth page,
then the first page has to be at 0x7f1edc79d000.

And sure enough there is a run starting at 0x7f1edc79d000 :

(gdb) p (arena_run_t *) 0x7f1edc79d000
$134 = (arena_run_t *) 0x7f1edc79d000
(gdb) p *$134
$135 = {
  bin = 0x7f22a1c3ece8, 
  nextind = 72, 
  nfree = 0
}

And the bin it points to:

(gdb) p $134.bin
$141 = (arena_bin_t *) 0x7f22a1c3ece8

Is the bin at index 11 in arena->bins:

(gdb) p &arena->bins[11]
$142 = (arena_bin_t *) 0x7f22a1c3ece8

So, this is a bin for allocations up to 224 bytes in size.

And, although this bin should have only 4 pages, it somehow seems to "extend"
into the fifth page, and overwrite another run.

The objects we are interested in have 0x01 as the first byte, and 
0x20 as the 26th byte, and the 9th and 10th byte combine to form an increasing
"block number".
Let us look for our special pattern in the 4th page:

We find many matches with the following matches seen towards the end:

0x7f1edc7a0660: 0x01    0x00    0x00    0x00    0x00    0x00    0x00    0x00
0x7f1edc7a0668: 0xd0    0x01    0x00    0x00    0x00    0x00    0x00    0x00
0x7f1edc7a0670: 0x00    0x00    0x00    0x00    0x00    0x00    0x00    0x00
0x7f1edc7a0678: 0x00    0x20    0x00    0x00    0x00    0xa1    0xbb    0x58


0x7f1edc7a0740: 0x01    0x00    0x00    0x00    0x00    0x00    0x00    0x00
0x7f1edc7a0748: 0xd1    0x01    0x00    0x00    0x00    0x00    0x00    0x00
0x7f1edc7a0750: 0x00    0x00    0x00    0x00    0x00    0x00    0x00    0x00
0x7f1edc7a0758: 0x00    0x20    0x00    0x00    0x00    0xa1    0xbb    0x58


0x7f1edc7a0820: 0x01    0x00    0x00    0x00    0x00    0x00    0x00    0x00
0x7f1edc7a0828: 0xd2    0x01    0x00    0x00    0x00    0x00    0x00    0x00
0x7f1edc7a0830: 0x00    0x00    0x00    0x00    0x00    0x00    0x00    0x00
0x7f1edc7a0838: 0x00    0x20    0x00    0x00    0x00    0x85    0xed    0x58

As can be seen the "block number" that can be seen on the second lines is 
increasing like 0x1d0, 0x1d1, 0x1d2.

And we know that the first match of this pattern on 
the page at 0x7f1edc7a1000 ( the "fifth" page ), is at 0x7f1edc7a1380, and the
"blockNumber" we see in it is 0x1d3:

0x7f1edc7a1380: 0x01    0x00    0x00    0x00    0x00    0x00    0x00    0x00
0x7f1edc7a1388: 0xd3    0x01    0x00    0x00    0x00    0x00    0x00    0x00
0x7f1edc7a1390: 0x00    0x00    0x00    0x00    0x00    0x00    0x00    0x00
0x7f1edc7a1398: 0x00    0x20    0x00    0x00    0x00    0x00    0x00    0x00

So, there is a progression of block numbers from the fourth page to the "fifth"
page of the run.

And we know that what we are calling the "fifth" page here is the beginning of
another run for allocations up to size 32:


(gdb) p run
$31 = (arena_run_t *) 0x7f1edc7a1000 ==> this is the "fifth" page

(gdb) p run->bin
$33 = (arena_bin_t *) 0x7f22a1c3e790

(gdb) p arena->bins[2]
$37 = (arena_bin_t *) 0x7f22a1c3e790  ==> same as run->bin in above line

(gdb) p arena_bin_info[2]
$38 = {
  reg_size = 32, 
  redzone_size = 0, 
  reg_interval = 32, 
  run_size = 4096, 
  nregs = 126, 
  bitmap_offset = 16, 
  bitmap_info = {
    nbits = 126, 
    nlevels = 2, 
    levels = {{
        group_offset = 0
      }, {
        group_offset = 2
      }, {
        group_offset = 3
      }, {
        group_offset = 0
      }, {
        group_offset = 0
      }}
  }, 
  ctx0_offset = 0, 
  reg0_offset = 64
}

So, when allocations in this "fifth" page are de-allocated, they are put in the
tcache bin for allocations of size between 16 and 32. That explains the wrong
tcache bin that we see. 

Please let us know what you think.

Thanks a lot!

--Chaitanya.
________________________________________
From: Chaitanya Patti
Sent: Tuesday, April 19, 2016 11:26 AM
To: Jason Evans
Cc: jemalloc-discuss at canonware.com
Subject: Re: One run extending into another in jemalloc-3.5.1 ?

Let us say "run1" has extended into "run2". Are you saying that the metadata of run1 has been corrupted by a double free ?

--Chaitanya

> On Apr 19, 2016, at 10:01, Jason Evans <jasone at canonware.com> wrote:
>
>> On Apr 18, 2016, at 11:12 PM, Chaitanya Patti <cpatti at tintri.com> wrote:
>> I am debugging a memory de-allocation issue. We are using jemalloc version 3.5.1. It looks like a run with reg_size 224 and total size of 4 pages has "extended" into an adjacent run, and corrupted the adjacent run. Has such an issue been seen before ?
>
> That usually means that a double free corrupted metadata for the adjacent run.  If you have a repeatable test case, try running with a debug build of jemalloc, and disable tcache, so that assertions immediately detect double frees.
>
> Jason

From daniel at rethinkdb.com  Fri Apr 22 16:38:47 2016
From: daniel at rethinkdb.com (Daniel Mewes)
Date: Fri, 22 Apr 2016 16:38:47 -0700
Subject: Regression: jemalloc >= 4.0 use munmap() even when configured with
	--disable-munmap
Message-ID: <CAC5vWnqNLZi-ifvAnHBK5Gg8Y8t-nHo5mA3sy-D03zOTrW49JQ@mail.gmail.com>

Thank you for your great work on jemalloc first of all.

In jemalloc 3.0, this patch added the `--disable-munmap` option and
disabled the use of `munmap` on Linux by default:
https://github.com/jemalloc/jemalloc/commit/59ae2766af88bad07ac721c4ee427b171e897bcb

It looks like jemalloc starting with version 4.0 makes use of `munmap` even
when `--disable-munmap` is specified. From what I can tell, `chunk_map.c`
honors the `config_munmap` flag, but the function `page_unmap` in `pages.c`
ignores it (this code appears to be new in jemalloc 4?).

We are using jemalloc for RethinkDB and would like to upgrade to version
4.1 because we think that it fixes some bugs that our users have run into.
However it causes a regression for
https://github.com/rethinkdb/rethinkdb/issues/3516 :
"<jemalloc>: Error in munmap(): Cannot allocate memory"


- Daniel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20160422/8b6876fb/attachment.html>

From jasone at canonware.com  Fri Apr 22 21:24:30 2016
From: jasone at canonware.com (Jason Evans)
Date: Fri, 22 Apr 2016 21:24:30 -0700
Subject: Regression: jemalloc >= 4.0 use munmap() even when configured
	with --disable-munmap
In-Reply-To: <CAC5vWnqNLZi-ifvAnHBK5Gg8Y8t-nHo5mA3sy-D03zOTrW49JQ@mail.gmail.com>
References: <CAC5vWnqNLZi-ifvAnHBK5Gg8Y8t-nHo5mA3sy-D03zOTrW49JQ@mail.gmail.com>
Message-ID: <DC691FE4-629F-403F-B1AE-B18A5B33DB6A@canonware.com>

On Apr 22, 2016, at 4:38 PM, Daniel Mewes <daniel at rethinkdb.com> wrote:
> In jemalloc 3.0, this patch added the `--disable-munmap` option and disabled the use of `munmap` on Linux by default: https://github.com/jemalloc/jemalloc/commit/59ae2766af88bad07ac721c4ee427b171e897bcb
> 
> It looks like jemalloc starting with version 4.0 makes use of `munmap` even when `--disable-munmap` is specified. From what I can tell, `chunk_map.c` honors the `config_munmap` flag, but the function `page_unmap` in `pages.c` ignores it (this code appears to be new in jemalloc 4?).
> 
> We are using jemalloc for RethinkDB and would like to upgrade to version 4.1 because we think that it fixes some bugs that our users have run into.
> However it causes a regression for https://github.com/rethinkdb/rethinkdb/issues/3516 :
> "<jemalloc>: Error in munmap(): Cannot allocate memory"

pages_unmap() is used to trim mappings so that what remains is chunk-aligned, regardless of whether --disable-munmap is specified.  jemalloc 3.x has similar code that calls munmap().  I don't see anything in what you're describing that is particular to jemalloc 4.x.  Are you able to determine anything else about the failure?  Its' extremely unusual for munmap() to fail (I've not seen it happen since ~2005 during initial development), so I'm guessing there's a memory corruption issue of some sort, whether due to a bug in jemalloc or RethinkDB.

Thanks,
Jason

From daniel at rethinkdb.com  Fri Apr 22 22:22:38 2016
From: daniel at rethinkdb.com (Daniel Mewes)
Date: Fri, 22 Apr 2016 22:22:38 -0700
Subject: Regression: jemalloc >= 4.0 use munmap() even when configured
	with --disable-munmap
In-Reply-To: <DC691FE4-629F-403F-B1AE-B18A5B33DB6A@canonware.com>
References: <CAC5vWnqNLZi-ifvAnHBK5Gg8Y8t-nHo5mA3sy-D03zOTrW49JQ@mail.gmail.com>
	<DC691FE4-629F-403F-B1AE-B18A5B33DB6A@canonware.com>
Message-ID: <CAC5vWnr=65vhKftvS3Xe4kyFCYJXfSEeVNS6s2Z1P1+juZODcA@mail.gmail.com>

Hi Jason,

thank you for your reply.

The reason for the failing `munmap` appears to be that we hit the kernel's
`max_map_count` limit.

I can reproduce the issue very quickly by reducing the limit through `echo
16000 > /proc/sys/vm/max_map_count`, and it disappears in our tests when
increasing it to something like `echo 131060 > /proc/sys/vm/max_map_count`.
The default value is 65530 I believe.

We used to see this behavior in jemalloc 2.x, but didn't see it in 3.x
anymore. It now re-appeared somewhere between 3.6 and 4.1.

It looks like I looked at the wrong place when I checked the jemalloc 3.6
code for comparison earlier today, and I can now see that the same code was
indeed there just in a different file (`chunk_mmap.c`). Thanks for
clarifying this.

So it seems that the difference between 3.6 and 4.1 must be caused by
something else then, and we might just have been lucky that the particular
behavior of jemalloc 3 didn't trigger the issue for our workload.

Do you think the allocator should handle reaching the map_count limit and
somehow deal with it gracefully (if that's even possible)? Or should we
just advise our users to raise the kernel limit, or alternatively try to
change RethinkDB's allocation patterns to avoid hitting it?

I can try to come up with a small test case to specifically reproduce this
issue later.

- Daniel



On Fri, Apr 22, 2016 at 9:24 PM, Jason Evans <jasone at canonware.com> wrote:

> On Apr 22, 2016, at 4:38 PM, Daniel Mewes <daniel at rethinkdb.com> wrote:
> > In jemalloc 3.0, this patch added the `--disable-munmap` option and
> disabled the use of `munmap` on Linux by default:
> https://github.com/jemalloc/jemalloc/commit/59ae2766af88bad07ac721c4ee427b171e897bcb
> >
> > It looks like jemalloc starting with version 4.0 makes use of `munmap`
> even when `--disable-munmap` is specified. From what I can tell,
> `chunk_map.c` honors the `config_munmap` flag, but the function
> `page_unmap` in `pages.c` ignores it (this code appears to be new in
> jemalloc 4?).
> >
> > We are using jemalloc for RethinkDB and would like to upgrade to version
> 4.1 because we think that it fixes some bugs that our users have run into.
> > However it causes a regression for
> https://github.com/rethinkdb/rethinkdb/issues/3516 :
> > "<jemalloc>: Error in munmap(): Cannot allocate memory"
>
> pages_unmap() is used to trim mappings so that what remains is
> chunk-aligned, regardless of whether --disable-munmap is specified.
> jemalloc 3.x has similar code that calls munmap().  I don't see anything in
> what you're describing that is particular to jemalloc 4.x.  Are you able to
> determine anything else about the failure?  Its' extremely unusual for
> munmap() to fail (I've not seen it happen since ~2005 during initial
> development), so I'm guessing there's a memory corruption issue of some
> sort, whether due to a bug in jemalloc or RethinkDB.
>
> Thanks,
> Jason
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20160422/9fcc0f5c/attachment.html>

From jasone at canonware.com  Fri Apr 22 22:41:04 2016
From: jasone at canonware.com (Jason Evans)
Date: Fri, 22 Apr 2016 22:41:04 -0700
Subject: Regression: jemalloc >= 4.0 use munmap() even when configured
	with --disable-munmap
In-Reply-To: <CAC5vWnr=65vhKftvS3Xe4kyFCYJXfSEeVNS6s2Z1P1+juZODcA@mail.gmail.com>
References: <CAC5vWnqNLZi-ifvAnHBK5Gg8Y8t-nHo5mA3sy-D03zOTrW49JQ@mail.gmail.com>
	<DC691FE4-629F-403F-B1AE-B18A5B33DB6A@canonware.com>
	<CAC5vWnr=65vhKftvS3Xe4kyFCYJXfSEeVNS6s2Z1P1+juZODcA@mail.gmail.com>
Message-ID: <C60E1A16-D79E-433C-B353-CF595BB5E57B@canonware.com>

On Apr 22, 2016, at 10:22 PM, Daniel Mewes <daniel at rethinkdb.com> wrote:
> The reason for the failing `munmap` appears to be that we hit the kernel's `max_map_count` limit.
> 
> I can reproduce the issue very quickly by reducing the limit through `echo 16000 > /proc/sys/vm/max_map_count`, and it disappears in our tests when increasing it to something like `echo 131060 > /proc/sys/vm/max_map_count`. The default value is 65530 I believe.
> 
> We used to see this behavior in jemalloc 2.x, but didn't see it in 3.x anymore. It now re-appeared somewhere between 3.6 and 4.1.

Version 4 switched to per arena management of huge allocations, and along with that completely independent trees of cached chunks.  For many workloads this means increased virtual memory usage, since cached chunks can't migrate among arenas.  I have plans to reduce the impact somewhat by decreasing the number of arenas by 4X, but the independence of arenas' mappings has numerous advantages that I plan to leverage more over time.

> Do you think the allocator should handle reaching the map_count limit and somehow deal with it gracefully (if that's even possible)? Or should we just advise our users to raise the kernel limit, or alternatively try to change RethinkDB's allocation patterns to avoid hitting it?

I'm surprised you're hitting this, because the normal mode of operation is for jemalloc's chunk allocation to get almost all contiguous mappings, which means very few distinct kernel VM map entries.  Is it possible that RethinkDB is routinely calling mmap() and interspersing mappings that are not a multiple of the chunk size?  One would hope that the kernel could densely pack such small mappings in the existing gaps between jemalloc's chunks, but unfortunately Linux uses fragile heuristics to find available virtual memory (the exact problem that --disable-munmap works around).

To your question about making jemalloc gracefully deal with munmap() failure, it seems likely that mmap() is in imminent danger of failing under these conditions, so there's not much that can be done.  In fact, jemalloc only aborts if the abort option is set to true (the default for debug builds), so the error message jemalloc is printing probably doesn't directly correspond to a crash.

As a workaround, you could substantially increase the chunk size (e.g. MALLOC_CONF=lg_chunk:30), but better would be to diagnose and address whatever is causing the terrible VM map fragmentation.

Thanks,
Jason

From daniel at rethinkdb.com  Fri Apr 22 22:56:22 2016
From: daniel at rethinkdb.com (Daniel Mewes)
Date: Fri, 22 Apr 2016 22:56:22 -0700
Subject: Regression: jemalloc >= 4.0 use munmap() even when configured
	with --disable-munmap
In-Reply-To: <C60E1A16-D79E-433C-B353-CF595BB5E57B@canonware.com>
References: <CAC5vWnqNLZi-ifvAnHBK5Gg8Y8t-nHo5mA3sy-D03zOTrW49JQ@mail.gmail.com>
	<DC691FE4-629F-403F-B1AE-B18A5B33DB6A@canonware.com>
	<CAC5vWnr=65vhKftvS3Xe4kyFCYJXfSEeVNS6s2Z1P1+juZODcA@mail.gmail.com>
	<C60E1A16-D79E-433C-B353-CF595BB5E57B@canonware.com>
Message-ID: <CAC5vWnrAe6ROtrF3GoFsjF0iBewfdRkQ9YPfusvQdrZL7sy2-Q@mail.gmail.com>

Thanks Jason, that's very helpful. I'll see if changing the `lg_chunk`
parameter changes anything.

In the meantime I found out that one likely reason for why RethinkDB
generates so many discontiguous VM mappings is because of our use of
`mprotect`. We use `mprotect` to install "guard pages" in heap-allocated
coroutine stacks, of which there can be quite a few under some workloads.


I now believe that this isn't really a jemalloc issue per se. At the very
least there are other factors involved.
We'll look into this more on our side, but I consider this a false alarm
for now.
Thanks for taking your time for explaining things here!

- Daniel

On Fri, Apr 22, 2016 at 10:41 PM, Jason Evans <jasone at canonware.com> wrote:

> On Apr 22, 2016, at 10:22 PM, Daniel Mewes <daniel at rethinkdb.com> wrote:
> > The reason for the failing `munmap` appears to be that we hit the
> kernel's `max_map_count` limit.
> >
> > I can reproduce the issue very quickly by reducing the limit through
> `echo 16000 > /proc/sys/vm/max_map_count`, and it disappears in our tests
> when increasing it to something like `echo 131060 >
> /proc/sys/vm/max_map_count`. The default value is 65530 I believe.
> >
> > We used to see this behavior in jemalloc 2.x, but didn't see it in 3.x
> anymore. It now re-appeared somewhere between 3.6 and 4.1.
>
> Version 4 switched to per arena management of huge allocations, and along
> with that completely independent trees of cached chunks.  For many
> workloads this means increased virtual memory usage, since cached chunks
> can't migrate among arenas.  I have plans to reduce the impact somewhat by
> decreasing the number of arenas by 4X, but the independence of arenas'
> mappings has numerous advantages that I plan to leverage more over time.
>
> > Do you think the allocator should handle reaching the map_count limit
> and somehow deal with it gracefully (if that's even possible)? Or should we
> just advise our users to raise the kernel limit, or alternatively try to
> change RethinkDB's allocation patterns to avoid hitting it?
>
> I'm surprised you're hitting this, because the normal mode of operation is
> for jemalloc's chunk allocation to get almost all contiguous mappings,
> which means very few distinct kernel VM map entries.  Is it possible that
> RethinkDB is routinely calling mmap() and interspersing mappings that are
> not a multiple of the chunk size?  One would hope that the kernel could
> densely pack such small mappings in the existing gaps between jemalloc's
> chunks, but unfortunately Linux uses fragile heuristics to find available
> virtual memory (the exact problem that --disable-munmap works around).
>
> To your question about making jemalloc gracefully deal with munmap()
> failure, it seems likely that mmap() is in imminent danger of failing under
> these conditions, so there's not much that can be done.  In fact, jemalloc
> only aborts if the abort option is set to true (the default for debug
> builds), so the error message jemalloc is printing probably doesn't
> directly correspond to a crash.
>
> As a workaround, you could substantially increase the chunk size (e.g.
> MALLOC_CONF=lg_chunk:30), but better would be to diagnose and address
> whatever is causing the terrible VM map fragmentation.
>
> Thanks,
> Jason
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20160422/ec01d00f/attachment-0001.html>

From bradley at mit.edu  Sat Apr 23 03:18:19 2016
From: bradley at mit.edu (Bradley C. Kuszmaul)
Date: Sat, 23 Apr 2016 06:18:19 -0400
Subject: Regression: jemalloc >= 4.0 use munmap() even when configured
	with --disable-munmap
In-Reply-To: <C60E1A16-D79E-433C-B353-CF595BB5E57B@canonware.com>
References: <CAC5vWnqNLZi-ifvAnHBK5Gg8Y8t-nHo5mA3sy-D03zOTrW49JQ@mail.gmail.com>
	<DC691FE4-629F-403F-B1AE-B18A5B33DB6A@canonware.com>
	<CAC5vWnr=65vhKftvS3Xe4kyFCYJXfSEeVNS6s2Z1P1+juZODcA@mail.gmail.com>
	<C60E1A16-D79E-433C-B353-CF595BB5E57B@canonware.com>
Message-ID: <CAKSyJXfn5q=gSWxzGBFyGQx3u4GJOwdExgLqfJzPiuKCkbsujA@mail.gmail.com>

If you are interested in making sure that jemalloc doesn't run into kernel
mapping limits, even in the face of an application that's doing additional
mappings and mprotects, then you can reduce the number of kernel mappings
used by jemalloc.  Increasing the chunk size is a blunt instrument, but
with a little work on the jemalloc internals one can do a little better.
One possible way to avoid this kind of mmap failure is to use a little bit
more virtual memory. If you were to allocate multiple chunks with a single
mmap(), you could guarantee that fewer kernel mappings are used, for
example.

To be concrete, suppose that the first 4MiB chunk you mmap 4MiB, but the
2nd one you mmap 8MiB (and save the extra chunks somewhere for later).  The
3rd chunk would come out of the saved extra chunks, and the 4th chunk would
call for for a 16MiB mmap.  You would be guaranteed to use at most 2X the
virtual memory and you would have at most log_2 N kernel mappings.

If you don't want to waste 2X virtual memory, you can reduce it by mapping
several times of a given size.  For example if you were to mmap(4MiB)
twice, then mmap(8Mib) twice, then mmap(16MiB) twice, and so forth, then
the VM space used would be at most 1.5X.  If you wanted to get the VM-space
overhead down to 10% you could do 10 mappings of a given size before
doubling the mapping size.  In general if you perform K mappings of a given
size before doubling the mapping size, you would have VM space overhead of
at most (1+K)/K, and the number of kernel mappings used by jemalloc would
be O(K log N) for N mappings.

By using other formulas for the amount of overallocation, you can further
reduce the VM overhead at the expense of having more mappings.  For example
you could make the $i$th mmap call allocate $i$ chunks.  This results in
overhead that is proportional to the square root of the number of mappings
done so far.  So at the beginning the overhead is large (a full factor of
two), but after running for a while, the overhead as a fraction becomes
smaller.  The number of kernel mappings would be bounded by O(\sqrt{N}) in
this case.

This approach doesn't stop the application from running out of mappings,
but it does avoid implicating jemalloc.

-Bradley

On Sat, Apr 23, 2016 at 1:41 AM, Jason Evans <jasone at canonware.com> wrote:

> On Apr 22, 2016, at 10:22 PM, Daniel Mewes <daniel at rethinkdb.com> wrote:
> > The reason for the failing `munmap` appears to be that we hit the
> kernel's `max_map_count` limit.
> >
> > I can reproduce the issue very quickly by reducing the limit through
> `echo 16000 > /proc/sys/vm/max_map_count`, and it disappears in our tests
> when increasing it to something like `echo 131060 >
> /proc/sys/vm/max_map_count`. The default value is 65530 I believe.
> >
> > We used to see this behavior in jemalloc 2.x, but didn't see it in 3.x
> anymore. It now re-appeared somewhere between 3.6 and 4.1.
>
> Version 4 switched to per arena management of huge allocations, and along
> with that completely independent trees of cached chunks.  For many
> workloads this means increased virtual memory usage, since cached chunks
> can't migrate among arenas.  I have plans to reduce the impact somewhat by
> decreasing the number of arenas by 4X, but the independence of arenas'
> mappings has numerous advantages that I plan to leverage more over time.
>
> > Do you think the allocator should handle reaching the map_count limit
> and somehow deal with it gracefully (if that's even possible)? Or should we
> just advise our users to raise the kernel limit, or alternatively try to
> change RethinkDB's allocation patterns to avoid hitting it?
>
> I'm surprised you're hitting this, because the normal mode of operation is
> for jemalloc's chunk allocation to get almost all contiguous mappings,
> which means very few distinct kernel VM map entries.  Is it possible that
> RethinkDB is routinely calling mmap() and interspersing mappings that are
> not a multiple of the chunk size?  One would hope that the kernel could
> densely pack such small mappings in the existing gaps between jemalloc's
> chunks, but unfortunately Linux uses fragile heuristics to find available
> virtual memory (the exact problem that --disable-munmap works around).
>
> To your question about making jemalloc gracefully deal with munmap()
> failure, it seems likely that mmap() is in imminent danger of failing under
> these conditions, so there's not much that can be done.  In fact, jemalloc
> only aborts if the abort option is set to true (the default for debug
> builds), so the error message jemalloc is printing probably doesn't
> directly correspond to a crash.
>
> As a workaround, you could substantially increase the chunk size (e.g.
> MALLOC_CONF=lg_chunk:30), but better would be to diagnose and address
> whatever is causing the terrible VM map fragmentation.
>
> Thanks,
> Jason
> _______________________________________________
> jemalloc-discuss mailing list
> jemalloc-discuss at canonware.com
> http://www.canonware.com/mailman/listinfo/jemalloc-discuss
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://www.canonware.com/pipermail/jemalloc-discuss/attachments/20160423/d0ed2b91/attachment.html>

