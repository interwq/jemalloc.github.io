<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> double allocations:
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20double%20allocations%3A&In-Reply-To=%3C67bddff80bf736cece4cb86c3e814f0a%40earth.cs.uml.edu%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000632.html">
   <LINK REL="Next"  HREF="000634.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>double allocations:</H1>
    <B>bill</B> 
    <A HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20double%20allocations%3A&In-Reply-To=%3C67bddff80bf736cece4cb86c3e814f0a%40earth.cs.uml.edu%3E"
       TITLE="double allocations:">bill at cs.uml.edu
       </A><BR>
    <I>Mon Sep 23 09:59:09 PDT 2013</I>
    <P><UL>
        <LI>Previous message: <A HREF="000632.html">[PATCH] malloc_conf_init: revert errno value when readlink(2) fail.
</A></li>
        <LI>Next message: <A HREF="000634.html">double allocations:
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#633">[ date ]</a>
              <a href="thread.html#633">[ thread ]</a>
              <a href="subject.html#633">[ subject ]</a>
              <a href="author.html#633">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>I've noticed that an allocation request for a large chunk of memory 
(128GB) results in two calls to pages_map() (in src/chunk_mmap.c), 
consuming 2x the VM I requested.  In a 64 bit world this is not a big 
problem, but I've recoded pages_map() to force allocation from an mmap'd 
ssd (instead of swap anonymous mmap), and it's forcing me to run out of 
backing store.  The issue I would like to understand is why pages_map() 
is called twice with separate requests for the single 128GB jemalloc() 
that I'm doing in my application.  The first allocation is followed by a 
call to pages_unmap(), but with an unmap size of 0 bytes, leaving it 
fully mapped, while the second allocation (which is slightly larger than 
128GB) is trimmed to exactly 128GB by 2 subsequent pages_unmap() calls.  
This behavior seems very strange to me, and any explanation would be 
appreciated.
Bill


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000632.html">[PATCH] malloc_conf_init: revert errno value when readlink(2) fail.
</A></li>
	<LI>Next message: <A HREF="000634.html">double allocations:
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#633">[ date ]</a>
              <a href="thread.html#633">[ thread ]</a>
              <a href="subject.html#633">[ subject ]</a>
              <a href="author.html#633">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">More information about the jemalloc-discuss
mailing list</a><br>
</body></html>
