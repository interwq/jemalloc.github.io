<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> double allocations:
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20double%20allocations%3A&In-Reply-To=%3C731A6984-DC62-4A07-B574-02AE7E331E8C%40canonware.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000633.html">
   <LINK REL="Next"  HREF="000635.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>double allocations:</H1>
    <B>Jason Evans</B> 
    <A HREF="mailto:jemalloc-discuss%40canonware.com?Subject=Re%3A%20double%20allocations%3A&In-Reply-To=%3C731A6984-DC62-4A07-B574-02AE7E331E8C%40canonware.com%3E"
       TITLE="double allocations:">jasone at canonware.com
       </A><BR>
    <I>Mon Sep 23 18:13:06 PDT 2013</I>
    <P><UL>
        <LI>Previous message: <A HREF="000633.html">double allocations:
</A></li>
        <LI>Next message: <A HREF="000635.html">Invitation to connect on LinkedIn
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#634">[ date ]</a>
              <a href="thread.html#634">[ thread ]</a>
              <a href="subject.html#634">[ subject ]</a>
              <a href="author.html#634">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On Sep 23, 2013, at 9:59 AM, bill &lt;<A HREF="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">bill at cs.uml.edu</A>&gt; wrote:
&gt;<i> I've noticed that an allocation request for a large chunk of memory (128GB) results in two calls to pages_map() (in src/chunk_mmap.c), consuming 2x the VM I requested.  In a 64 bit world this is not a big problem, but I've recoded pages_map() to force allocation from an mmap'd ssd (instead of swap anonymous mmap), and it's forcing me to run out of backing store.  The issue I would like to understand is why pages_map() is called twice with separate requests for the single 128GB jemalloc() that I'm doing in my application.  The first allocation is followed by a call to pages_unmap(), but with an unmap size of 0 bytes, leaving it fully mapped, while the second allocation (which is slightly larger than 128GB) is trimmed to exactly 128GB by 2 subsequent pages_unmap() calls.  This behavior seems very strange to me, and any explanation would be appreciated.
</I>
It sounds like the first time pages_map() is called, it returns a result that isn't adequately aligned.  The second time, extra space is allocated so that the result can be trimmed to alignment boundaries.

You say that the interposed call to pages_unmap() receives a size of 0.  Assuming the call is coming from chunk_alloc_mmap(), I see no way that can happen.  There was a bug in a ~3-year-old version of jemalloc of this nature, but I hope you're using a more modern version.  Also of relevance: the SSD backing feature you added existed in 2.x versions of jemalloc, but I removed it because no one ever claimed to have found it useful.

Jason
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000633.html">double allocations:
</A></li>
	<LI>Next message: <A HREF="000635.html">Invitation to connect on LinkedIn
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#634">[ date ]</a>
              <a href="thread.html#634">[ thread ]</a>
              <a href="subject.html#634">[ subject ]</a>
              <a href="author.html#634">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.canonware.com/mailman/listinfo/jemalloc-discuss">More information about the jemalloc-discuss
mailing list</a><br>
</body></html>
